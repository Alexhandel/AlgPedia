ABOUT
In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.
FULL TEXT
In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.


The algorithm was named after Nicholas Metropolis, who was an author along with Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller of the 1953 paper Equation of State Calculations by Fast Computing Machines which first proposed the algorithm for the case of symmetrical proposal distributions, and W. K. Hastings who extended it to the more general case in 1970.[1] There is controversy over the credit for discovery of the algorithm. Edward Teller states in his memoirs that the five authors of the 1953 paper worked together for "days (and nights)".[2] M. Rosenbluth, in an oral history recorded shortly before his death[3] credits E. Teller with posing the original problem, himself with solving it, and A.W. Rosenbluth (his wife) with programming the computer. According to M. Rosenbluth, neither Metropolis nor A.H. Teller participated in any way. Rosenbluth's account of events is supported by other contemporary recollections.[4] According to Roy Glauber and Emilio Segrè, the original algorithm was invented by Enrico Fermi and reinvented by Stan Ulam.
The Metropolis–Hastings algorithm can draw samples from any probability distribution P(x), provided you can compute the value of a function f(x) that is proportional to the density of P. The lax requirement that f(x) should be merely proportional to the density, rather than exactly equal to it, makes the Metropolis–Hastings algorithm particularly useful, because calculating the necessary normalization factor is often extremely difficult in practice.
The Metropolis–Hastings algorithm works by generating a sequence of sample values in such a way that, as more and more sample values are produced, the distribution of values more closely approximates the desired distribution, P(x). These sample values are produced iteratively, with the distribution of the next sample being dependent only on the current sample value (thus making the sequence of samples into a Markov chain). Specifically, at each iteration, the algorithm picks a candidate for the next sample value based on the current sample value. Then, with some probability, the candidate is either accepted (in which case the candidate value is used in the next iteration) or rejected (in which case the candidate value is discarded, and current value is reused in the next iteration)−the probability of acceptance is determined by comparing the values of the function f(x) of the current and candidate sample values with respect to the desired distribution P(x).
For the purpose of illustration, the Metropolis algorithm, a special case of the Metropolis–Hastings algorithm where the proposal function is symmetric, is described below.
Metropolis algorithm (symmetric proposal distribution)
Let f(x) be a function that is proportional to the desired probability distribution P(x) (a.k.a. a target distribution).
This algorithm proceeds by randomly attempting to move about the sample space, sometimes accepting the moves and sometimes remaining in place. Note that the acceptance ratio 



α


{\displaystyle \alpha }

 indicates how probable the new proposed sample is with respect to the current sample, according to the distribution 




P
(
x
)



{\displaystyle \displaystyle P(x)}

. If we attempt to move to a point that is more probable than the existing point (i.e. a point in a higher-density region of 




P
(
x
)



{\displaystyle \displaystyle P(x)}

), we will always accept the move. However, if we attempt to move to a less probable point, we will sometimes reject the move, and the more the relative drop in probability, the more likely we are to reject the new point. Thus, we will tend to stay in (and return large numbers of samples from) high-density regions of 




P
(
x
)



{\displaystyle \displaystyle P(x)}

, while only occasionally visiting low-density regions. Intuitively, this is why this algorithm works, and returns samples that follow the desired distribution 




P
(
x
)



{\displaystyle \displaystyle P(x)}

.
Compared with an algorithm like adaptive rejection sampling[5] that directly generates independent samples from a distribution, Metropolis–Hastings and other MCMC algorithms have a number of disadvantages:
On the other hand, most simple rejection sampling methods suffer from the "curse of dimensionality", where the probability of rejection increases exponentially as a function of the number of dimensions. Metropolis–Hastings, along with other MCMC methods, do not have this problem to such a degree, and thus are often the only solutions available when the number of dimensions of the distribution to be sampled is high. As a result, MCMC methods are often the methods of choice for producing samples from hierarchical Bayesian models and other high-dimensional statistical models used nowadays in many disciplines.
In multivariate distributions, the classic Metropolis–Hastings algorithm as described above involves choosing a new multi-dimensional sample point. When the number of dimensions is high, finding the right jumping distribution to use can be difficult, as the different individual dimensions behave in very different ways, and the jumping width (see above) must be "just right" for all dimensions at once to avoid excessively slow mixing. An alternative approach that often works better in such situations, known as Gibbs sampling, involves choosing a new sample for each dimension separately from the others, rather than choosing a sample for all dimensions at once. This is especially applicable when the multivariate distribution is composed out of a set of individual random variables in which each variable is conditioned on only a small number of other variables, as is the case in most typical hierarchical models. The individual variables are then sampled one at a time, with each variable conditioned on the most recent values of all the others. Various algorithms can be used to choose these individual samples, depending on the exact form of the multivariate distribution: some possibilities are the adaptive rejection sampling methods,[5][6][7][8] the adaptive rejection Metropolis sampling algorithm[9] or its improvements[10][11] (see matlab code), a simple one-dimensional Metropolis–Hastings step, or slice sampling.
The purpose of the Metropolis–Hastings algorithm is to generate a collection of states according to a desired distribution P(x). To accomplish this, the algorithm uses a Markov process which asymptotically reaches a unique stationary distribution π(x) such that π(x)=P(x) .[12]
A Markov process is uniquely defined by its transition probabilities, 



P
(

x
′


|

x
)


{\displaystyle P(x'|x)}

, the probability of transitioning from any given state, x, to any other given state, x'. It has a unique stationary distribution π(x) when the following two conditions are met:[12]
The Metropolis–Hastings algorithm involves designing a Markov process (by constructing transition probabilities) which fulfils the two above conditions, such that its stationary distribution π(x) is chosen to be P(x). The derivation of the algorithm starts with the condition of detailed balance:




P
(

x
′


|

x
)
P
(
x
)
=
P
(
x

|


x
′

)
P
(

x
′

)


{\displaystyle P(x'|x)P(x)=P(x|x')P(x')}


which is re-written as







P
(

x
′


|

x
)


P
(
x

|


x
′

)



=



P
(

x
′

)


P
(
x
)





{\displaystyle {\frac {P(x'|x)}{P(x|x')}}={\frac {P(x')}{P(x)}}}

.
The approach is to separate the transition in two sub-steps; the proposal and the acceptance-rejection. The proposal distribution 




g
(

x
′


|

x
)



{\displaystyle \displaystyle g(x'|x)}

 is the conditional probability of proposing a state x' given x, and the acceptance distribution 




A
(

x
′


|

x
)



{\displaystyle \displaystyle A(x'|x)}

 the conditional probability to accept the proposed state x'. The transition probability can be written as the product of them:




P
(

x
′


|

x
)
=
g
(

x
′


|

x
)
A
(

x
′


|

x
)


{\displaystyle P(x'|x)=g(x'|x)A(x'|x)}

 .
Inserting this relation in the previous equation, we have







A
(

x
′


|

x
)


A
(
x

|


x
′

)



=



P
(

x
′

)


P
(
x
)






g
(
x

|


x
′

)


g
(

x
′


|

x
)





{\displaystyle {\frac {A(x'|x)}{A(x|x')}}={\frac {P(x')}{P(x)}}{\frac {g(x|x')}{g(x'|x)}}}

 .
The next step in the derivation is to choose an acceptance that fulfils the condition above. One common choice is the Metropolis choice:




A
(

x
′


|

x
)
=
min

(
1
,



P
(

x
′

)


P
(
x
)






g
(
x

|


x
′

)


g
(

x
′


|

x
)



)



{\displaystyle A(x'|x)=\min \left(1,{\frac {P(x')}{P(x)}}{\frac {g(x|x')}{g(x'|x)}}\right)}


i.e., we always accept when the acceptance is bigger than 1, and we reject accordingly when the acceptance is smaller than 1. This is the required quantity for the algorithm.
The Metropolis–Hastings algorithm thus consists in the following:
The saved states are in principle drawn from the distribution 



P
(
x
)


{\displaystyle P(x)}

, as step 4 ensures they are de-correlated. The value of T must be chosen according to different factors such as the proposal distribution and, formally, it has to be of the order of the autocorrelation time of the Markov process.[13]
It is important to notice that it is not clear, in a general problem, which distribution 




g
(

x
′


|

x
)



{\displaystyle \displaystyle g(x'|x)}

 one should use; it is a free parameter of the method which has to be adjusted to the particular problem in hand.
A common use of Metropolis–Hastings algorithm is to compute an integral. Specifically, consider a space 



Ω
⊂

R



{\displaystyle \Omega \subset \mathbb {R} }

 and a probability distribution P(x) over 



Ω


{\displaystyle \Omega }

, 



x
∈
Ω


{\displaystyle x\in \Omega }

. Metropolis-Hastings can estimate an integral of the form of
where A(x) is an arbitrary function of interest. For example, consider a statistic E(x) and its probability distribution P(E), which is a marginal distribution. Suppose that the goal is to estimate P(E) for E on the tail of P(E). Formally, P(E) can be written as
and, thus, estimating P(E) can be accomplished by estimating the expected value of the indicator function 




A

E


(
x
)
≡


1


E


(
x
)


{\displaystyle A_{E}(x)\equiv \mathbf {1} _{E}(x)}

, which is 1 when 



E
(
x
)
∈
[
E
,
E
+
Δ
E
]


{\displaystyle E(x)\in [E,E+\Delta E]}

 and zero otherwise. Because E is on the tail of P(E), the probability to draw a state x with E(x) on the tail of P(E) is proportional to P(E), which is small by definition. Metropolis-Hastings can be used here to sample (rare) states more likely and thus increase the number of samples used to estimate P(E) on the tails. This can be done e.g. by using a sampling distribution 



π
(
x
)


{\displaystyle \pi (x)}

 to favor those states (e.g. 



π
(
x
)
∝

e

a
E




{\displaystyle \pi (x)\propto e^{aE}}

 with a>0).
Suppose the most recent value sampled is 




x

t





{\displaystyle x_{t}\,}

. To follow the Metropolis–Hastings algorithm, we next draw a new proposal state 




x
′




{\displaystyle x'\,}

 with probability density 



g
(

x
′


|


x

t


)



{\displaystyle g(x'|x_{t})\,}

, and calculate a value
where
is the probability (e.g., Bayesian posterior) ratio between the proposed sample 




x
′




{\displaystyle x'\,}

 and the previous sample 




x

t





{\displaystyle x_{t}\,}

, and
is the ratio of the proposal density in two directions (from 




x

t





{\displaystyle x_{t}\,}

 to 




x
′




{\displaystyle x'\,}

 and vice versa). This is equal to 1 if the proposal density is symmetric. Then the new state 





x

t
+
1





{\displaystyle \displaystyle x_{t+1}}

 is chosen according to the following rules.
The Markov chain is started from an arbitrary initial value 





x

0





{\displaystyle \displaystyle x_{0}}

 and the algorithm is run for many iterations until this initial state is "forgotten". These samples, which are discarded, are known as burn-in. The remaining set of accepted values of 



x


{\displaystyle x}

 represent a sample from the distribution 



P
(
x
)


{\displaystyle P(x)}

.
The algorithm works best if the proposal density matches the shape of the target distribution 




P
(
x
)



{\displaystyle \displaystyle P(x)}

 from which direct sampling is difficult, that is 



g
(

x
′


|


x

t


)
≈
P
(

x
′

)




{\displaystyle g(x'|x_{t})\approx P(x')\,\!}

. If a Gaussian proposal density 




g



{\displaystyle \displaystyle g}

 is used the variance parameter 





σ

2





{\displaystyle \displaystyle \sigma ^{2}}

 has to be tuned during the burn-in period. This is usually done by calculating the acceptance rate, which is the fraction of proposed samples that is accepted in a window of the last 




N



{\displaystyle \displaystyle N}

 samples. The desired acceptance rate depends on the target distribution, however it has been shown theoretically that the ideal acceptance rate for a one-dimensional Gaussian distribution is approx 50%, decreasing to approx 23% for an 




N



{\displaystyle \displaystyle N}

-dimensional Gaussian target distribution.[14]
If 





σ

2





{\displaystyle \displaystyle \sigma ^{2}}

 is too small the chain will mix slowly (i.e., the acceptance rate will be high but successive samples will move around the space slowly and the chain will converge only slowly to 




P
(
x
)



{\displaystyle \displaystyle P(x)}

). On the other hand, if 





σ

2





{\displaystyle \displaystyle \sigma ^{2}}

 is too large the acceptance rate will be very low because the proposals are likely to land in regions of much lower probability density, so 





a

1





{\displaystyle \displaystyle a_{1}}

 will be very small and again the chain will converge very slowly.