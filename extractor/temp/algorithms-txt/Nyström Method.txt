ABOUT
In numerical analysis, the Nyström method[1] or quadrature method seeks the numerical solution of an integral equation by replacing the integral with a representative weighted sum. The continuous problem is broken into 



n


{\displaystyle n}

 discrete intervals, quadrature or numerical integration determines the weights and locations of representative points for the integral. The discrete problem to be solved is now a system of linear equations with n equations and n unknowns.
FULL TEXT
In numerical analysis, the Nyström method[1] or quadrature method seeks the numerical solution of an integral equation by replacing the integral with a representative weighted sum. The continuous problem is broken into 



n


{\displaystyle n}

 discrete intervals, quadrature or numerical integration determines the weights and locations of representative points for the integral. The discrete problem to be solved is now a system of linear equations with n equations and n unknowns.
From the n solved points the function value at other points is interpolated consistent with the chosen quadrature method. Depending on the original problem and the choice of quadrature the problem may be ill-conditioned.
Since the linear equations require 



O
(

n

3


)


{\displaystyle O(n^{3})}

 operations to solve, hence high-order quadrature rules perform better because low-order quadrature rules require large 



n


{\displaystyle n}

 for a given accuracy. Gaussian quadrature is normally a good choice for smooth, non-singular problems.


Applying this to the inhomogeneous Fredholm equation of the second kind
results in
The most important step of the Nyström method is sampling, because different sampled landmark points give different approximations of the original matrix.
Uniform sampling without replacement: is the most used approach for this purpose, where every point has the same probability of being included in the sample.
MSSS is an non-uniform sampling approach that considers both the variance and the similarity of the data distribution in its sampling data, which approximately maximizes the determinant of the reduced similarity matrix that represents the mutual similarities between sampled data points. It is shown in[2] that this approach increase the speed of the clustering on large datasets.