{
    "about": "The Berndt\u2013Hall\u2013Hall\u2013Hausman (BHHH) algorithm is a numerical optimization algorithm similar to the Gauss\u2013Newton algorithm. It is named after the four originators: Ernst R. Berndt, B. Hall, Robert Hall, and Jerry Hausman.", 
    "name": "Berndt\u2013Hall\u2013Hall\u2013Hausman Algorithm", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "The Berndt\u2013Hall\u2013Hall\u2013Hausman (BHHH) algorithm is a numerical optimization algorithm similar to the Gauss\u2013Newton algorithm. It is named after the four originators: Ernst R. Berndt, B. Hall, Robert Hall, and Jerry Hausman.\nIf a nonlinear model is fitted to the data one often needs to estimate coefficients through optimization. A number of optimisation algorithms have the following general structure. Suppose that the function to be optimized is Q(\u03b2). Then the algorithms are iterative, defining a sequence of approximations, \u03b2k given by\nwhere \n\n\n\n\n\u03b2\n\nk\n\n\n\n\n{\\displaystyle \\beta _{k}}\n\n is the parameter estimate at step k, and \n\n\n\n\n\u03bb\n\nk\n\n\n\n\n{\\displaystyle \\lambda _{k}}\n\n is a parameter (called step size) which partly determines the particular algorithm. For the BHHH algorithm \u03bbk is determined by calculations within a given iterative step, involving a line-search until a point \u03b2k+1 is found satisfying certain criteria. In addition, for the BHHH algorithm, Q has the form\nand A is calculated using\nIn other cases, e.g. Newton\u2013Raphson, \n\n\n\n\nA\n\nk\n\n\n\n\n{\\displaystyle A_{k}}\n\n can have other forms. The BHHH algorithm has the advantage that, if certain conditions apply, convergence of the iterative procedure is guaranteed.[citation needed]", 
    "dbpedia_url": "http://dbpedia.org/resource/Berndt\u2013Hall\u2013Hall\u2013Hausman_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Berndt\u2013Hall\u2013Hall\u2013Hausman_algorithm\n"
}