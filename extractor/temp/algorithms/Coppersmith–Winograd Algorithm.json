{
    "about": "In linear algebra, the Coppersmith\u2013Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known matrix multiplication algorithm until 2010. It can multiply two \n\n\n\nn\n\u00d7\nn\n\n\n{\\displaystyle n\\times n}\n\n matrices in \n\n\n\nO\n(\n\nn\n\n2.375477\n\n\n)\n\n\n{\\displaystyle O(n^{2.375477})}\n\n time [1] (see Big O notation). This is an improvement over the na\u00efve \n\n\n\nO\n(\n\nn\n\n3\n\n\n)\n\n\n{\\displaystyle O(n^{3})}\n\n time algorithm and the \n\n\n\nO\n(\n\nn\n\n2.807355\n\n\n)\n\n\n{\\displaystyle O(n^{2.807355})}\n\n time Strassen algorithm. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical.[2] It is possible to improve the exponent further; however, the exponent must be at least 2 (because an \n\n\n\nn\n\u00d7\nn\n\n\n{\\displaystyle n\\times n}\n\n matrix has \n\n\n\n\nn\n\n2\n\n\n\n\n{\\displaystyle n^{2}}\n\n values, and all of them have to be read at least once to calculate the exact result).", 
    "name": "Coppersmith\u2013Winograd Algorithm", 
    "classification": "Matrix Multiplication Algorithms", 
    "full_text": "In linear algebra, the Coppersmith\u2013Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known matrix multiplication algorithm until 2010. It can multiply two \n\n\n\nn\n\u00d7\nn\n\n\n{\\displaystyle n\\times n}\n\n matrices in \n\n\n\nO\n(\n\nn\n\n2.375477\n\n\n)\n\n\n{\\displaystyle O(n^{2.375477})}\n\n time [1] (see Big O notation). This is an improvement over the na\u00efve \n\n\n\nO\n(\n\nn\n\n3\n\n\n)\n\n\n{\\displaystyle O(n^{3})}\n\n time algorithm and the \n\n\n\nO\n(\n\nn\n\n2.807355\n\n\n)\n\n\n{\\displaystyle O(n^{2.807355})}\n\n time Strassen algorithm. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical.[2] It is possible to improve the exponent further; however, the exponent must be at least 2 (because an \n\n\n\nn\n\u00d7\nn\n\n\n{\\displaystyle n\\times n}\n\n matrix has \n\n\n\n\nn\n\n2\n\n\n\n\n{\\displaystyle n^{2}}\n\n values, and all of them have to be read at least once to calculate the exact result).\nIn 2010, Andrew Stothers gave an improvement to the algorithm, \n\n\n\nO\n(\n\nn\n\n2.374\n\n\n)\n.\n\n\n{\\displaystyle O(n^{2.374}).}\n\n[3][4] In 2011, Virginia Williams combined a mathematical short-cut from Stothers' paper with her own insights and automated optimization on computers, improving the bound to \n\n\n\nO\n(\n\nn\n\n2.3728642\n\n\n)\n.\n\n\n{\\displaystyle O(n^{2.3728642}).}\n\n[5] In 2014, Fran\u00e7ois Le Gall simplified the methods of Williams and obtained an improved bound of \n\n\n\nO\n(\n\nn\n\n2.3728639\n\n\n)\n.\n\n\n{\\displaystyle O(n^{2.3728639}).}\n\n[6]\nThe Coppersmith\u2013Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. However, unlike the Strassen algorithm, it is not used in practice because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.[7]\nHenry Cohn, Robert Kleinberg, Bal\u00e1zs Szegedy and Chris Umans have re-derived the Coppersmith\u2013Winograd algorithm using a group-theoretic construction. They also showed that either of two different conjectures would imply that the optimal exponent of matrix multiplication is 2, as has long been suspected. However, they were not able to formulate a specific solution leading to a better running-time than Coppersmith-Winograd at the time.[8]", 
    "dbpedia_url": "http://dbpedia.org/resource/Coppersmith\u2013Winograd_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Coppersmith\u2013Winograd_algorithm\n"
}