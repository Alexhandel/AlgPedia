{
    "about": "In statistics, an expectation\u2013maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.", 
    "name": "Expectation\u2013Maximization Algorithm", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "In statistics, an expectation\u2013maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\n\n\nThe EM algorithm was explained and given its name in a classic 1977 paper by Arthur Dempster, Nan Laird, and Donald Rubin.[1] They pointed out that the method had been \"proposed many times in special circumstances\" by earlier authors. A very detailed treatment of the EM method for exponential families was published by Rolf Sundberg in his thesis and several papers[2][3][4] following his collaboration with Per Martin-L\u00f6f and Anders Martin-L\u00f6f.[5][6][7][8][9][10][11] The Dempster-Laird-Rubin paper in 1977 generalized the method and sketched a convergence analysis for a wider class of problems. Regardless of earlier inventions, the innovative Dempster-Laird-Rubin paper in the Journal of the Royal Statistical Society received an enthusiastic discussion at the Royal Statistical Society meeting with Sundberg calling the paper \"brilliant\". The Dempster-Laird-Rubin paper established the EM method as an important tool of statistical analysis.\nThe convergence analysis of the Dempster-Laird-Rubin paper was flawed and a correct convergence analysis was published by C.F. Jeff Wu in 1983.[12] Wu's proof established the EM method's convergence outside of the exponential family, as claimed by Dempster-Laird-Rubin.[12]\nThe EM algorithm is used to find (locally) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. That is, either missing values exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.\nFinding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. In statistical models with latent variables, this is usually impossible. Instead, the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa, but substituting one set of equations into the other produces an unsolvable equation.\nThe EM algorithm proceeds from the observation that the following is a way to solve these two sets of equations numerically. One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points. It's not obvious that this will work at all, but it can be proven that in this context it does, and that the derivative of the likelihood is (arbitrarily close to) zero at that point, which in turn means that the point is either a maximum or a saddle point.[12] In general, multiple maxima may occur, with no guarantee that the global maximum will be found. Some likelihoods also have singularities in them, i.e., nonsensical maxima. For example, one of the solutions that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points.\nGiven the statistical model which generates a set \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n of observed data, a set of unobserved latent data or missing values \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n, and a vector of unknown parameters \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n, along with a likelihood function \n\n\n\nL\n(\n\n\u03b8\n\n;\n\nX\n\n,\n\nZ\n\n)\n=\np\n(\n\nX\n\n,\n\nZ\n\n\n|\n\n\n\u03b8\n\n)\n\n\n{\\displaystyle L({\\boldsymbol {\\theta }};\\mathbf {X} ,\\mathbf {Z} )=p(\\mathbf {X} ,\\mathbf {Z} |{\\boldsymbol {\\theta }})}\n\n, the maximum likelihood estimate (MLE) of the unknown parameters is determined by the marginal likelihood of the observed data\nHowever, this quantity is often intractable (e.g. if \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n is a sequence of events, so that the number of values grows exponentially with the sequence length, making the exact calculation of the sum extremely difficult).\nThe EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying these two steps:\nIn typical models to which EM is applied:\nHowever, it is possible to apply EM to other sorts of models.\nThe motive is as follows. If the value of the parameters \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n is known, usually the value of the latent variables \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n can be found by maximizing the log-likelihood over all possible values of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n, either simply by iterating over \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n or through an algorithm such as the Viterbi algorithm for hidden Markov models. Conversely, if we know the value of the latent variables \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n, we can find an estimate of the parameters \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n fairly easily, typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values, or some function of the values, of the points in each group. This suggests an iterative algorithm, in the case where both \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n and \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n are unknown:\nThe algorithm as just described monotonically approaches a local minimum of the cost function, and is commonly called hard EM. The k-means algorithm is an example of this class of algorithms.\nHowever, somewhat better methods exist. Rather than making a hard choice for \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n given the current parameter values and averaging only over the set of data points associated with some value of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n, instead, determine the probability of each possible value of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n for each data point, and then use the probabilities associated with some value of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n to compute a weighted average over the whole set of data points. The resulting algorithm is commonly called soft EM, and is the type of algorithm normally associated with EM. The counts used to compute these weighted averages are called soft counts (as opposed to the hard counts used in a hard-EM-type algorithm such as k-means). The probabilities computed for \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n are posterior probabilities and are what is computed in the E step. The soft counts used to compute new parameter values are what is computed in the M step.\nSpeaking of an expectation (E) step is a bit of a misnomer. What is calculated in the first step are the fixed, data-dependent parameters of the function Q. Once the parameters of Q are known, it is fully determined and is maximized in the second (M) step of an EM algorithm.\nAlthough an EM iteration does increase the observed data (i.e., marginal) likelihood function, no guarantee exists that the sequence converges to a maximum likelihood estimator. For multimodal distributions, this means that an EM algorithm may converge to a local maximum of the observed data likelihood function, depending on starting values. A variety of heuristic or metaheuristic approaches exist to escape a local maximum, such as random-restart hill climbing (starting with several different random initial estimates \u03b8(t)), or applying simulated annealing methods.\nEM is especially useful when the likelihood is an exponential family: the E step becomes the sum of expectations of sufficient statistics, and the M step involves maximizing a linear function. In such a case, it is usually possible to derive closed-form expression updates for each step, using the Sundberg formula (published by Rolf Sundberg using unpublished results of Per Martin-L\u00f6f and Anders Martin-L\u00f6f).[3][4][7][8][9][10][11]\nThe EM method was modified to compute maximum a posteriori (MAP) estimates for Bayesian inference in the original paper by Dempster, Laird, and Rubin.\nOther methods exist to find maximum likelihood estimates, such as gradient descent, conjugate gradient, or variants of the Gauss\u2013Newton algorithm. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.\nExpectation-maximization works to improve \n\n\n\nQ\n(\n\n\u03b8\n\n\n|\n\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\n\n{\\displaystyle Q({\\boldsymbol {\\theta }}|{\\boldsymbol {\\theta }}^{(t)})}\n\n rather than directly improving \n\n\n\nlog\n\u2061\np\n(\n\nX\n\n\n|\n\n\n\u03b8\n\n)\n\n\n{\\displaystyle \\log p(\\mathbf {X} |{\\boldsymbol {\\theta }})}\n\n. Here is shown that improvements to the former imply improvements to the latter.[13]\nFor any \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n with non-zero probability \n\n\n\np\n(\n\nZ\n\n\n|\n\n\nX\n\n,\n\n\u03b8\n\n)\n\n\n{\\displaystyle p(\\mathbf {Z} |\\mathbf {X} ,{\\boldsymbol {\\theta }})}\n\n, we can write\nWe take the expectation over possible values of the unknown data \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n under the current parameter estimate \n\n\n\n\n\u03b8\n\n(\nt\n)\n\n\n\n\n{\\displaystyle \\theta ^{(t)}}\n\n by multiplying both sides by \n\n\n\np\n(\n\nZ\n\n\n|\n\n\nX\n\n,\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\n\n{\\displaystyle p(\\mathbf {Z} |\\mathbf {X} ,{\\boldsymbol {\\theta }}^{(t)})}\n\n and summing (or integrating) over \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbf {Z} }\n\n. The left-hand side is the expectation of a constant, so we get:\nwhere \n\n\n\nH\n(\n\n\u03b8\n\n\n|\n\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\n\n{\\displaystyle H({\\boldsymbol {\\theta }}|{\\boldsymbol {\\theta }}^{(t)})}\n\n is defined by the negated sum it is replacing. This last equation holds for any value of \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n including \n\n\n\n\n\u03b8\n\n=\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}={\\boldsymbol {\\theta }}^{(t)}}\n\n,\nand subtracting this last equation from the previous equation gives\nHowever, Gibbs' inequality tells us that \n\n\n\nH\n(\n\n\u03b8\n\n\n|\n\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\u2265\nH\n(\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n\n|\n\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\n\n{\\displaystyle H({\\boldsymbol {\\theta }}|{\\boldsymbol {\\theta }}^{(t)})\\geq H({\\boldsymbol {\\theta }}^{(t)}|{\\boldsymbol {\\theta }}^{(t)})}\n\n, so we can conclude that\nIn words, choosing \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n to improve \n\n\n\nQ\n(\n\n\u03b8\n\n\n|\n\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\n\n{\\displaystyle Q({\\boldsymbol {\\theta }}|{\\boldsymbol {\\theta }}^{(t)})}\n\n beyond \n\n\n\nQ\n(\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n\n|\n\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\n\n{\\displaystyle Q({\\boldsymbol {\\theta }}^{(t)}|{\\boldsymbol {\\theta }}^{(t)})}\n\n can not cause \n\n\n\nlog\n\u2061\np\n(\n\nX\n\n\n|\n\n\n\u03b8\n\n)\n\n\n{\\displaystyle \\log p(\\mathbf {X} |{\\boldsymbol {\\theta }})}\n\n to decrease below \n\n\n\nlog\n\u2061\np\n(\n\nX\n\n\n|\n\n\n\n\u03b8\n\n\n(\nt\n)\n\n\n)\n\n\n{\\displaystyle \\log p(\\mathbf {X} |{\\boldsymbol {\\theta }}^{(t)})}\n\n, and so the marginal likelihood of the data is non-decreasing.\nThe EM algorithm can be viewed as two alternating maximization steps, that is, as an example of coordinate ascent.[14][15] Consider the function:\nwhere q is an arbitrary probability distribution over the unobserved data z and H(q) is the entropy of the distribution q. This function can be written as\nwhere \n\n\n\n\np\n\nZ\n\n|\n\nX\n\n\n(\n\u22c5\n\n|\n\nx\n;\n\u03b8\n)\n\n\n{\\displaystyle p_{Z|X}(\\cdot |x;\\theta )}\n\n is the conditional distribution of the unobserved data given the observed data \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\n\nD\n\nK\nL\n\n\n\n\n{\\displaystyle D_{KL}}\n\n is the Kullback\u2013Leibler divergence.\nThen the steps in the EM algorithm may be viewed as:\nEM is frequently used for data clustering in machine learning and computer vision. In natural language processing, two prominent instances of the algorithm are the Baum-Welch algorithm and the inside-outside algorithm for unsupervised induction of probabilistic context-free grammars.\nIn psychometrics, EM is almost indispensable for estimating item parameters and latent abilities of item response theory models.\nWith the ability to deal with missing data and observe unidentified variables, EM is becoming a useful tool to price and manage risk of a portfolio.[ref?]\nThe EM algorithm (and its faster variant ordered subset expectation maximization) is also widely used in medical image reconstruction, especially in positron emission tomography and single photon emission computed tomography. See below for other faster variants of EM.\nIn structural engineering, the Structural Identification using Expectation Maximization (STRIDE) [16] algorithm is an output-only method for identifying natural vibration properties of a structural system using sensor data (see Operational Modal Analysis).\nA Kalman filter is typically used for on-line state estimation and a minimum-variance smoother may be employed for off-line or batch state estimation. However, these minimum-variance solutions require estimates of the state-space model parameters. EM algorithms can be used for solving joint state and parameter estimation problems.\nFiltering and smoothing EM algorithms arise by repeating this two-step procedure:\nSuppose that a Kalman filter or minimum-variance smoother operates on noisy measurements of a single-input-single-output system. An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation\nwhere \n\n\n\n\n\n\n\nx\n^\n\n\n\n\nk\n\n\n\n\n{\\displaystyle {\\hat {x}}_{k}}\n\n are scalar output estimates calculated by a filter or a smoother from N scalar measurements \n\n\n\n\n\nz\n\nk\n\n\n\n\n\n{\\displaystyle {z_{k}}}\n\n. Similarly, for a first-order auto-regressive process, an updated process noise variance estimate can be calculated by\nwhere \n\n\n\n\n\n\n\nx\n^\n\n\n\n\nk\n\n\n\n\n{\\displaystyle {\\hat {x}}_{k}}\n\n and \n\n\n\n\n\n\n\nx\n^\n\n\n\n\nk\n+\n1\n\n\n\n\n{\\displaystyle {\\hat {x}}_{k+1}}\n\n are scalar state estimates calculated by a filter or a smoother. The updated model coefficient estimate is obtained via\nThe convergence of parameter estimates such as those above are well studied.[17][18][19]\nA number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm, such as those using conjugate gradient and modified Newton's methods (Newton\u2013Raphson).[20] Also, EM can be used with constrained estimation methods.\nExpectation conditional maximization (ECM) replaces each M step with a sequence of conditional maximization (CM) steps in which each parameter \u03b8i is maximized individually, conditionally on the other parameters remaining fixed.[21]\nThis idea is further extended in generalized expectation maximization (GEM) algorithm, in which is sought only an increase in the objective function F for both the E step and M step under the alternative description.[14] GEM is further developed in a distributed environment and shows promising results.[22]\nIt is also possible to consider the EM algorithm as a subclass of the MM (Majorize/Minimize or Minorize/Maximize, depending on context) algorithm,[23] and therefore use any machinery developed in the more general case.\nThe Q-function used in the EM algorithm is based on the log likelihood. Therefore, it is regarded as the log-EM algorithm. The use of the log likelihood can be generalized to that of the \u03b1-log likelihood ratio. Then, the \u03b1-log likelihood ratio of the observed data can be exactly expressed as equality by using the Q-function of the \u03b1-log likelihood ratio and the \u03b1-divergence. Obtaining this Q-function is a generalized E step. Its maximization is a generalized M step. This pair is called the \u03b1-EM algorithm [24] which contains the log-EM algorithm as its subclass. Thus, the \u03b1-EM algorithm by Yasuo Matsuyama is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The \u03b1-EM shows faster convergence than the log-EM algorithm by choosing an appropriate \u03b1. The \u03b1-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm \u03b1-HMM. [25]\nEM is a partially non-Bayesian, maximum likelihood method. Its final result gives a probability distribution over the latent variables (in the Bayesian style) together with a point estimate for \u03b8 (either a maximum likelihood estimate or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over \u03b8 and the latent variables. The Bayesian approach to inference is simply to treat \u03b8 as another latent variable. In this paradigm, the distinction between the E and M steps disappears. If using the factorized Q approximation as described above (variational Bayes), solving can iterate over each latent variable (now including \u03b8) and optimize them one at a time. Now, k steps per iteration are needed, where k is the number of latent variables. For graphical models this is easy to do as each variable's new Q depends only on its Markov blanket, so local message passing can be used for efficient inference.\nIn information geometry, the E step and the M step are interpreted as projections under dual affine connections, called the e-connection and the m-connection; the Kullback\u2013Leibler divergence can also be understood in these terms.\nLet \n\n\n\n\nx\n\n=\n(\n\n\nx\n\n\n1\n\n\n,\n\n\nx\n\n\n2\n\n\n,\n\u2026\n,\n\n\nx\n\n\nn\n\n\n)\n\n\n{\\displaystyle \\mathbf {x} =(\\mathbf {x} _{1},\\mathbf {x} _{2},\\ldots ,\\mathbf {x} _{n})}\n\n be a sample of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n independent observations from a mixture of two multivariate normal distributions of dimension \n\n\n\nd\n\n\n{\\displaystyle d}\n\n, and let \n\n\n\n\nz\n\n=\n(\n\nz\n\n1\n\n\n,\n\nz\n\n2\n\n\n,\n\u2026\n,\n\nz\n\nn\n\n\n)\n\n\n{\\displaystyle \\mathbf {z} =(z_{1},z_{2},\\ldots ,z_{n})}\n\n be the latent variables that determine the component from which the observation originates.[15]\nwhere\nThe aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:\nwhere the incomplete-data likelihood function is\nand the complete-data likelihood function is\nor\nwhere \n\n\n\n\nI\n\n\n\n{\\displaystyle \\mathbb {I} }\n\n is an indicator function and \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is the probability density function of a multivariate normal.\nTo see the last equality, then for each i all indicators \n\n\n\n\nI\n\n(\n\nz\n\ni\n\n\n=\nj\n)\n\n\n{\\displaystyle \\mathbb {I} (z_{i}=j)}\n\n are equal to zero, except for one which is equal to one. The inner sum thus reduces to one term.\nGiven our current estimate of the parameters \u03b8(t), the conditional distribution of the Zi is determined by Bayes theorem to be the proportional height of the normal density weighted by \u03c4:\nThese are called the \"membership probabilities\" which are normally considered the output of the E step (although this is not the Q function of below).\nThis E step corresponds with this function for Q:\nThis full conditional expectation does not need to be calculated in one step, because \u03c4 and \u03bc/\u03a3 appear in separate linear terms and can thus be maximized independently.\nQ(\u03b8|\u03b8(t)) being quadratic in form means that determining the maximizing values of \u03b8 is relatively straightforward. Also, \u03c4, (\u03bc1,\u03a31) and (\u03bc2,\u03a32) may all be maximized independently since they all appear in separate linear terms.\nTo begin, consider \u03c4, which has the constraint \u03c41 + \u03c42=1:\nThis has the same form as the MLE for the binomial distribution, so\nFor the next estimates of (\u03bc1,\u03a31):\nThis has the same form as a weighted MLE for a normal distribution, so\nand, by symmetry\nConclude the iterative process if \n\n\n\n\nE\n\nZ\n\n|\n\n\n\u03b8\n\n(\nt\n)\n\n\n,\n\nx\n\n\n\n[\nlog\n\u2061\nL\n(\n\n\u03b8\n\n(\nt\n)\n\n\n;\n\nx\n\n,\n\nZ\n\n)\n]\n\u2264\n\nE\n\nZ\n\n|\n\n\n\u03b8\n\n(\nt\n\u2212\n1\n)\n\n\n,\n\nx\n\n\n\n[\nlog\n\u2061\nL\n(\n\n\u03b8\n\n(\nt\n\u2212\n1\n)\n\n\n;\n\nx\n\n,\n\nZ\n\n)\n]\n+\n\u03f5\n\n\n{\\displaystyle E_{Z|\\theta ^{(t)},\\mathbf {x} }[\\log L(\\theta ^{(t)};\\mathbf {x} ,\\mathbf {Z} )]\\leq E_{Z|\\theta ^{(t-1)},\\mathbf {x} }[\\log L(\\theta ^{(t-1)};\\mathbf {x} ,\\mathbf {Z} )]+\\epsilon }\n\n for \n\n\n\n\u03f5\n\n\n{\\displaystyle \\epsilon }\n\n below some preset threshold.\nThe algorithm illustrated above can be generalized for mixtures of more than two multivariate normal distributions.\nThe EM algorithm has been implemented in the case where an underlying linear regression model exists explaining the variation of some quantity, but where the values actually observed are censored or truncated versions of those represented in the model.[26] Special cases of this model include censored or truncated observations from one normal distribution.[26]\nEM typically converges to a local optimum, not necessarily the global optimum, with no bound on the convergence rate in general. It is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima. Hence, a need exists for alternative methods for guaranteed learning, especially in the high-dimensional setting. Alternatives to EM exist with better guarantees for consistency, which are termed moment-based approaches or the so-called spectral techniques. Moment-based approaches to learning the parameters of a probabilistic model are of increasing interest recently since they enjoy guarantees such as global convergence under certain conditions unlike EM which is often plagued by the issue of getting stuck in local optima. Algorithms with guarantees for learning can be derived for a number of important models such as mixture models, HMMs etc. For these spectral methods, no spurious local optima occur, and the true parameters can be consistently estimated under some regularity conditions.", 
    "dbpedia_url": "http://dbpedia.org/resource/Expectation\u2013maximization_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Expectation\u2013maximization_algorithm\n"
}