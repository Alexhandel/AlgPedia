{
    "about": "The Frank\u2013Wolfe algorithm is an iterative first-order optimization algorithm for constrained convex optimization. Also known as the conditional gradient method,[1] reduced gradient algorithm and the convex combination algorithm, the method was originally proposed by Marguerite Frank and Philip Wolfe in\u00a01956.[2] In each iteration, the Frank\u2013Wolfe algorithm considers a linear approximation of the objective function, and moves towards a minimizer of this linear function (taken over the same domain).", 
    "name": "Frank\u2013Wolfe Algorithm", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "The Frank\u2013Wolfe algorithm is an iterative first-order optimization algorithm for constrained convex optimization. Also known as the conditional gradient method,[1] reduced gradient algorithm and the convex combination algorithm, the method was originally proposed by Marguerite Frank and Philip Wolfe in\u00a01956.[2] In each iteration, the Frank\u2013Wolfe algorithm considers a linear approximation of the objective function, and moves towards a minimizer of this linear function (taken over the same domain).\n\n\nSuppose \n\n\n\n\n\nD\n\n\n\n\n{\\displaystyle {\\mathcal {D}}}\n\n is a compact convex set in a vector space and \n\n\n\nf\n:\n\n\nD\n\n\n\u2192\n\nR\n\n\n\n{\\displaystyle f\\colon {\\mathcal {D}}\\to \\mathbb {R} }\n\n is a convex differentiable real-valued function. The Frank\u2013Wolfe algorithm solves the optimization problem\nWhile competing methods such as gradient descent for constrained optimization require a projection step back to the feasible set in each iteration, the Frank\u2013Wolfe algorithm only needs the solution of a linear problem over the same set in each iteration, and automatically stays in the feasible set.\nThe convergence of the Frank\u2013Wolfe algorithm is sublinear in general: the error in the objective function to the optimum is \n\n\n\nO\n(\n1\n\n/\n\nk\n)\n\n\n{\\displaystyle O(1/k)}\n\n after k iterations, so long as the gradient is Lipschitz continuous with respect to some norm. The same convergence rate can also be shown if the sub-problems are only solved approximately.[3]\nThe iterates of the algorithm can always be represented as a sparse convex combination of the extreme points of the feasible set, which has helped to the popularity of the algorithm for sparse greedy optimization in machine learning and signal processing problems,[4] as well as for example the optimization of minimum\u2013cost flows in transportation networks.[5]\nIf the feasible set is given by a set of linear constraints, then the subproblem to be solved in each iteration becomes a linear program.\nWhile the worst-case convergence rate with \n\n\n\nO\n(\n1\n\n/\n\nk\n)\n\n\n{\\displaystyle O(1/k)}\n\n can not be improved in general, faster convergence can be obtained for special problem classes, such as some strongly convex problems.[6]\nSince \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is convex, for any two points \n\n\n\n\nx\n\n,\n\ny\n\n\u2208\n\n\nD\n\n\n\n\n{\\displaystyle \\mathbf {x} ,\\mathbf {y} \\in {\\mathcal {D}}}\n\n we have:\nThis also holds for the (unknown) optimal solution \n\n\n\n\n\nx\n\n\n\u2217\n\n\n\n\n{\\displaystyle \\mathbf {x} ^{*}}\n\n. That is, \n\n\n\nf\n(\n\n\nx\n\n\n\u2217\n\n\n)\n\u2265\nf\n(\n\nx\n\n)\n+\n(\n\n\nx\n\n\n\u2217\n\n\n\u2212\n\nx\n\n\n)\n\nT\n\n\n\u2207\nf\n(\n\nx\n\n)\n\n\n{\\displaystyle f(\\mathbf {x} ^{*})\\geq f(\\mathbf {x} )+(\\mathbf {x} ^{*}-\\mathbf {x} )^{T}\\nabla f(\\mathbf {x} )}\n\n. The best lower bound with respect to a given point \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n is given by\nThe latter optimization problem is solved in every iteration of the Frank\u2013Wolfe algorithm, therefore the solution \n\n\n\n\n\ns\n\n\nk\n\n\n\n\n{\\displaystyle \\mathbf {s} _{k}}\n\n of the direction-finding subproblem of the \n\n\n\nk\n\n\n{\\displaystyle k}\n\n-th iteration can be used to determine increasing lower bounds \n\n\n\n\nl\n\nk\n\n\n\n\n{\\displaystyle l_{k}}\n\n during each iteration by setting \n\n\n\n\nl\n\n0\n\n\n=\n\u2212\n\u221e\n\n\n{\\displaystyle l_{0}=-\\infty }\n\n and\nSuch lower bounds on the unknown optimal value are important in practice because they can be used as a stopping criterion, and give an efficient certificate of the approximation quality in every iteration, since always \n\n\n\n\nl\n\nk\n\n\n\u2264\nf\n(\n\n\nx\n\n\n\u2217\n\n\n)\n\u2264\nf\n(\n\n\nx\n\n\nk\n\n\n)\n\n\n{\\displaystyle l_{k}\\leq f(\\mathbf {x} ^{*})\\leq f(\\mathbf {x} _{k})}\n\n.\nIt has been shown that this corresponding duality gap, that is the difference between \n\n\n\nf\n(\n\n\nx\n\n\nk\n\n\n)\n\n\n{\\displaystyle f(\\mathbf {x} _{k})}\n\n and the lower bound \n\n\n\n\nl\n\nk\n\n\n\n\n{\\displaystyle l_{k}}\n\n, decreases with the same convergence rate, i.e. \n\n\n\nf\n(\n\n\nx\n\n\nk\n\n\n)\n\u2212\n\nl\n\nk\n\n\n=\nO\n(\n1\n\n/\n\nk\n)\n.\n\n\n{\\displaystyle f(\\mathbf {x} _{k})-l_{k}=O(1/k).}\n\n", 
    "dbpedia_url": "http://dbpedia.org/resource/Frank\u2013Wolfe_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Frank\u2013Wolfe_algorithm\n"
}