{
    "about": "The Gauss\u2013Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss\u2013Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.", 
    "name": "Gauss\u2013Newton Algorithm", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "The Gauss\u2013Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss\u2013Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.\nNon-linear least squares problems arise for instance in non-linear regression, where parameters in a model are sought such that the model is in good agreement with available observations.\nThe method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton.\n\n\nGiven m functions r = (r1, \u2026, rm) (often called residuals) of n variables \u03b2\u00a0=\u00a0(\u03b21, \u2026, \u03b2n), with m\u00a0\u2265\u00a0n, the Gauss\u2013Newton algorithm iteratively finds the value of the variables which minimizes the sum of squares[1]\nStarting with an initial guess \n\n\n\n\n\n\u03b2\n\n\n(\n0\n)\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}^{(0)}}\n\n for the minimum, the method proceeds by the iterations\nwhere, if r and \u03b2 are column vectors, the entries of the Jacobian matrix are\nand the symbol \n\n\n\n\n\n\n\nT\n\n\n\n\n\n{\\displaystyle ^{\\mathsf {T}}}\n\n denotes the matrix transpose.\nIf m\u00a0=\u00a0n, the iteration simplifies to\nwhich is a direct generalization of Newton's method in one dimension.\nIn data fitting, where the goal is to find the parameters \u03b2 such that a given model function y\u00a0=\u00a0f(x, \u03b2) best fits some data points (xi, yi), the functions ri are the residuals\nThen, the Gauss-Newton method can be expressed in terms of the Jacobian Jf of the function f as\nThe assumption m\u00a0\u2265\u00a0n in the algorithm statement is necessary, as otherwise the matrix JrTJr is not invertible and the normal equations cannot be solved (at least uniquely).\nThe Gauss\u2013Newton algorithm can be derived by linearly approximating the vector of functions ri. Using Taylor's theorem, we can write at every iteration:\nwith \n\n\n\n\u0394\n=\n\n\u03b2\n\n\u2212\n\n\n\u03b2\n\n\ns\n\n\n.\n\n\n{\\displaystyle \\Delta ={\\boldsymbol {\\beta }}-{\\boldsymbol {\\beta }}^{s}.}\n\n The task of finding \u0394 minimizing the sum of squares of the right-hand side, i.e.,\nis a linear least squares problem, which can be solved explicitly, yielding the normal equations in the algorithm.\nThe normal equations are m linear simultaneous equations in the unknown increments, \u0394. They may be solved in one step, using Cholesky decomposition, or, better, the QR factorization of Jr. For large systems, an iterative method, such as the conjugate gradient method, may be more efficient. If there is a linear dependence between columns of Jr, the iterations will fail as JrTJr becomes singular.\nIn this example, the Gauss\u2013Newton algorithm will be used to fit a model to some data by minimizing the sum of squares of errors between the data and model's predictions.\nIn a biology experiment studying the relation between substrate concentration [S] and reaction rate in an enzyme-mediated reaction, the data in the following table were obtained.\nIt is desired to find a curve (model function) of the form\nthat fits best the data in the least squares sense, with the parameters \n\n\n\n\nV\n\nmax\n\n\n\n\n{\\displaystyle V_{\\text{max}}}\n\n and \n\n\n\n\nK\n\nM\n\n\n\n\n{\\displaystyle K_{M}}\n\n to be determined.\nDenote by \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n and \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n the value of [S] and the rate from the table, \n\n\n\ni\n=\n1\n,\n\u2026\n,\n7.\n\n\n{\\displaystyle i=1,\\dots ,7.}\n\n Let \n\n\n\n\n\u03b2\n\n1\n\n\n=\n\nV\n\nmax\n\n\n\n\n{\\displaystyle \\beta _{1}=V_{\\text{max}}}\n\n and \n\n\n\n\n\u03b2\n\n2\n\n\n=\n\nK\n\nM\n\n\n.\n\n\n{\\displaystyle \\beta _{2}=K_{M}.}\n\n We will find \n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n and \n\n\n\n\n\u03b2\n\n2\n\n\n\n\n{\\displaystyle \\beta _{2}}\n\n such that the sum of squares of the residuals\nis minimized.\nThe Jacobian \n\n\n\n\n\nJ\n\nr\n\n\n\n\n\n{\\displaystyle \\mathbf {J_{r}} }\n\n of the vector of residuals \n\n\n\n\nr\n\ni\n\n\n\n\n{\\displaystyle r_{i}}\n\n in respect to the unknowns \n\n\n\n\n\u03b2\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\n is an \n\n\n\n7\n\u00d7\n2\n\n\n{\\displaystyle 7\\times 2}\n\n matrix with the \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th row having the entries\nStarting with the initial estimates of \n\n\n\n\n\u03b2\n\n1\n\n\n=\n0.9\n\n\n{\\displaystyle \\beta _{1}=0.9}\n\n and \n\n\n\n\n\u03b2\n\n2\n\n\n=\n0.2\n\n\n{\\displaystyle \\beta _{2}=0.2}\n\n, after five iterations of the Gauss\u2013Newton algorithm the optimal values \n\n\n\n\n\n\n\n\u03b2\n^\n\n\n\n\n1\n\n\n=\n0.362\n\n\n{\\displaystyle {\\hat {\\beta }}_{1}=0.362}\n\n and \n\n\n\n\n\n\n\n\u03b2\n^\n\n\n\n\n2\n\n\n=\n0.556\n\n\n{\\displaystyle {\\hat {\\beta }}_{2}=0.556}\n\n are obtained. The sum of squares of residuals decreased from the initial value of 1.445 to 0.00784 after the fifth iteration. The plot in the figure on the right shows the curve determined by the model for the optimal parameters versus the observed data.\nIt can be shown[2] that the increment \u0394 is a descent direction for S, and, if the algorithm converges, then the limit is a stationary point of S. However, convergence is not guaranteed, not even local convergence as in Newton's method, or convergence under the usual Wolfe conditions .[3]\nThe rate of convergence of the Gauss\u2013Newton algorithm can approach quadratic.[4] The algorithm may converge slowly or not at all if the initial guess is far from the minimum or the matrix \n\n\n\n\n\nJ\n\nr\n\n\n\nT\n\n\n\n\nJ\n\nr\n\n\n\n\n\n{\\displaystyle \\mathbf {J_{r}^{\\mathsf {T}}J_{r}} }\n\n is ill-conditioned. For example, consider the problem with \n\n\n\nm\n=\n2\n\n\n{\\displaystyle m=2}\n\n equations and \n\n\n\nn\n=\n1\n\n\n{\\displaystyle n=1}\n\n variable, given by\nThe optimum is at \n\n\n\n\u03b2\n=\n0\n\n\n{\\displaystyle \\beta =0}\n\n. (Actually the optimum is at \n\n\n\n\u03b2\n=\n\u2212\n1\n\n\n{\\displaystyle \\beta =-1}\n\n for \n\n\n\n\u03bb\n=\n2\n\n\n{\\displaystyle \\lambda =2}\n\n, because \n\n\n\nS\n(\n0\n)\n=\n\n1\n\n2\n\n\n+\n(\n\u2212\n1\n\n)\n\n2\n\n\n=\n2\n\n\n{\\displaystyle S(0)=1^{2}+(-1)^{2}=2}\n\n, but \n\n\n\nS\n(\n\u2212\n1\n)\n=\n0\n\n\n{\\displaystyle S(-1)=0}\n\n.) If \n\n\n\n\u03bb\n=\n0\n\n\n{\\displaystyle \\lambda =0}\n\n then the problem is in fact linear and the method finds the optimum in one iteration. If |\u03bb| < 1 then the method converges linearly and the error decreases asymptotically with a factor |\u03bb| at every iteration. However, if |\u03bb| > 1, then the method does not even converge locally.[5]\nIn what follows, the Gauss\u2013Newton algorithm will be derived from Newton's method for function optimization via an approximation. As a consequence, the rate of convergence of the Gauss\u2013Newton algorithm can be quadratic under certain regularity conditions. In general (under weaker conditions), the convergence rate is linear.[6]\nThe recurrence relation for Newton's method for minimizing a function S of parameters, \n\n\n\n\n\u03b2\n\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}}\n\n, is\nwhere g denotes the gradient vector of S and H denotes the Hessian matrix of S. Since \n\n\n\nS\n=\n\n\u2211\n\ni\n=\n1\n\n\nm\n\n\n\nr\n\ni\n\n\n2\n\n\n\n\n{\\displaystyle S=\\sum _{i=1}^{m}r_{i}^{2}}\n\n, the gradient is given by\nElements of the Hessian are calculated by differentiating the gradient elements, \n\n\n\n\ng\n\nj\n\n\n\n\n{\\displaystyle g_{j}}\n\n, with respect to \n\n\n\n\n\u03b2\n\nk\n\n\n\n\n{\\displaystyle \\beta _{k}}\n\n\nThe Gauss\u2013Newton method is obtained by ignoring the second-order derivative terms (the second term in this expression). That is, the Hessian is approximated by\nwhere \n\n\n\n\nJ\n\ni\nj\n\n\n=\n\n\n\n\u2202\n\nr\n\ni\n\n\n\n\n\u2202\n\n\u03b2\n\nj\n\n\n\n\n\n\n\n{\\displaystyle J_{ij}={\\frac {\\partial r_{i}}{\\partial \\beta _{j}}}}\n\n are entries of the Jacobian Jr. The gradient and the approximate Hessian can be written in matrix notation as\nThese expressions are substituted into the recurrence relation above to obtain the operational equations\nConvergence of the Gauss\u2013Newton method is not guaranteed in all instances. The approximation\nthat needs to hold to be able to ignore the second-order derivative terms may be valid in two cases, for which convergence is to be expected.[7]\nWith the Gauss\u2013Newton method the sum of squares of the residuals S may not decrease at every iteration. However, since \u0394 is a descent direction, unless \n\n\n\nS\n(\n\n\n\u03b2\n\n\ns\n\n\n)\n\n\n{\\displaystyle S({\\boldsymbol {\\beta }}^{s})}\n\n is a stationary point, it holds that \n\n\n\nS\n(\n\n\n\u03b2\n\n\ns\n\n\n+\n\u03b1\n\u0394\n)\n<\nS\n(\n\n\n\u03b2\n\n\ns\n\n\n)\n\n\n{\\displaystyle S({\\boldsymbol {\\beta }}^{s}+\\alpha \\Delta )<S({\\boldsymbol {\\beta }}^{s})}\n\n for all sufficiently small \n\n\n\n\u03b1\n>\n0\n\n\n{\\displaystyle \\alpha >0}\n\n. Thus, if divergence occurs, one solution is to employ a fraction, \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n, of the increment vector, \u0394 in the updating formula:\nIn other words, the increment vector is too long, but it points in \"downhill\", so going just a part of the way will decrease the objective function S. An optimal value for \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n can be found by using a line search algorithm, that is, the magnitude of \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n is determined by finding the value that minimizes S, usually using a direct search method in the interval \n\n\n\n0\n<\n\u03b1\n<\n1\n\n\n{\\displaystyle 0<\\alpha <1}\n\n.\nIn cases where the direction of the shift vector is such that the optimal fraction, \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n, is close to zero, an alternative method for handling divergence is the use of the Levenberg\u2013Marquardt algorithm, also known as the \"trust region method\".[1] The normal equations are modified in such a way that the increment vector is rotated towards the direction of steepest descent,\nwhere D is a positive diagonal matrix. Note that when D is the identity matrix I and \n\n\n\n\u03bb\n\u2192\n+\n\u221e\n\n\n{\\displaystyle \\lambda \\to +\\infty }\n\n, then \n\n\n\n\u03bb\n\u0394\n=\n\u03bb\n\n\n(\n\n\nJ\n\nT\n\n\nJ\n\n+\n\u03bb\n\nI\n\n)\n\n\n\u2212\n1\n\n\n\n(\n\u2212\n\n\nJ\n\n\nT\n\n\n\nr\n\n)\n\n=\n\n(\n\nI\n\n\u2212\n\n\nJ\n\nT\n\n\nJ\n\n\n/\n\n\u03bb\n+\n\u22ef\n)\n\n\n(\n\u2212\n\n\nJ\n\n\nT\n\n\n\nr\n\n)\n\n\u2192\n\u2212\n\n\nJ\n\n\nT\n\n\n\nr\n\n\n\n{\\displaystyle \\lambda \\Delta =\\lambda \\left(\\mathbf {J^{T}J} +\\lambda \\mathbf {I} \\right)^{-1}\\left(-\\mathbf {J} ^{T}\\mathbf {r} \\right)=\\left(\\mathbf {I} -\\mathbf {J^{T}J} /\\lambda +\\cdots \\right)\\left(-\\mathbf {J} ^{T}\\mathbf {r} \\right)\\to -\\mathbf {J} ^{T}\\mathbf {r} }\n\n, therefore the direction of \u0394 approaches the direction of the negative gradient \n\n\n\n\u2212\n\n\nJ\n\n\nT\n\n\n\nr\n\n\n\n{\\displaystyle -\\mathbf {J} ^{T}\\mathbf {r} }\n\n.\nThe so-called Marquardt parameter, \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n, may also be optimized by a line search, but this is inefficient as the shift vector must be re-calculated every time \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n is changed. A more efficient strategy is this. When divergence occurs increase the Marquardt parameter until there is a decrease in S. Then, retain the value from one iteration to the next, but decrease it if possible until a cut-off value is reached when the Marquardt parameter can be set to zero; the minimization of S then becomes a standard Gauss\u2013Newton minimization.\nFor large scale optimization, the Gauss-Newton method is of special interest because it is often (though certainly not always) true that the matrix \n\n\n\n\n\nJ\n\n\n\nr\n\n\n\n\n\n{\\displaystyle \\mathbf {J} _{\\mathbf {r} }}\n\n is more sparse than the approximate Hessian \n\n\n\n\n\nJ\n\n\n\nr\n\n\n\n\nT\n\n\n\n\n\nJ\n\nr\n\n\n\n\n\n{\\displaystyle \\mathbf {J} _{\\mathbf {r} }^{\\mathsf {T}}\\mathbf {J_{r}} }\n\n. In such cases, the step calculation itself will typically need to be done with an approximate iterative method appropriate for large and sparse problems, such as the conjugate gradient method.\nIn order to make this kind of approach work, one needs at minimum an efficient method for computing the product\nfor some vector p. With sparse matrix storage, it is in general practical to store the rows of \n\n\n\n\n\nJ\n\n\n\nr\n\n\n\n\n\n{\\displaystyle \\mathbf {J} _{\\mathbf {r} }}\n\n in a compressed form (eg, without zero entries), making a direct computation of the above product tricky due to the transposition. However, if one defines ci as row i of the matrix \n\n\n\n\n\nJ\n\n\n\nr\n\n\n\n\n\n{\\displaystyle \\mathbf {J} _{\\mathbf {r} }}\n\n, the following simple relation holds\nso that every row contributes additively and independently to the product. In addition to respecting a practical sparse storage structure, this expression is well suited for parallel computations. Note that every row ci is the gradient of the corresponding residual ri; with this in mind, the formula above emphasizes the fact that residuals contribute to the problem independently of each other.\nIn a quasi-Newton method, such as that due to Davidon, Fletcher and Powell or Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS method) an estimate of the full Hessian, \n\n\n\n\n\n\n\n\u2202\n\n2\n\n\nS\n\n\n\u2202\n\n\u03b2\n\nj\n\n\n\u2202\n\n\u03b2\n\nk\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {\\partial ^{2}S}{\\partial \\beta _{j}\\partial \\beta _{k}}}}\n\n, is built up numerically using first derivatives \n\n\n\n\n\n\n\u2202\n\nr\n\ni\n\n\n\n\n\u2202\n\n\u03b2\n\nj\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {\\partial r_{i}}{\\partial \\beta _{j}}}}\n\n only so that after n refinement cycles the method closely approximates to Newton's method in performance. Note that quasi-Newton methods can minimize general real-valued functions, whereas Gauss-Newton, Levenberg-Marquardt, etc. fits only to nonlinear least-squares problems.\nAnother method for solving minimization problems using only first derivatives is gradient descent. However, this method does not take into account the second derivatives even approximately. Consequently, it is highly inefficient for many functions, especially if the parameters have strong interactions.\n", 
    "dbpedia_url": "http://dbpedia.org/resource/Gauss\u2013Newton_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Gauss\u2013Newton_algorithm\n"
}