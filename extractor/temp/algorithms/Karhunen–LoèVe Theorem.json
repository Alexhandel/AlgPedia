{
    "about": "In the theory of stochastic processes, the Karhunen\u2013Lo\u00e8ve theorem (named after Kari Karhunen and Michel Lo\u00e8ve), also known as the Kosambi\u2013Karhunen\u2013Lo\u00e8ve theorem[1][2] is a representation of a stochastic process as an infinite linear combination of orthogonal functions, analogous to a Fourier series representation of a function on a bounded interval. The transformation is also known as Hotelling Transform and Eigenvector Transform, and is closely related to Principal Component Analysis (PCA) technique widely used in image processing and in data analysis in many fields.[3]", 
    "name": "Karhunen\u2013Lo\u00e8Ve Theorem", 
    "classification": "Signal Processing", 
    "full_text": "In the theory of stochastic processes, the Karhunen\u2013Lo\u00e8ve theorem (named after Kari Karhunen and Michel Lo\u00e8ve), also known as the Kosambi\u2013Karhunen\u2013Lo\u00e8ve theorem[1][2] is a representation of a stochastic process as an infinite linear combination of orthogonal functions, analogous to a Fourier series representation of a function on a bounded interval. The transformation is also known as Hotelling Transform and Eigenvector Transform, and is closely related to Principal Component Analysis (PCA) technique widely used in image processing and in data analysis in many fields.[3]\nStochastic processes given by infinite series of this form were first[4] considered by Damodar Dharmananda Kosambi.[5] There exist many such expansions of a stochastic process: if the process is indexed over [a, b], any orthonormal basis of L2([a, b]) yields an expansion thereof in that form. The importance of the Karhunen\u2013Lo\u00e8ve theorem is that it yields the best such basis in the sense that it minimizes the total mean squared error.\nIn contrast to a Fourier series where the coefficients are fixed numbers and the expansion basis consists of sinusoidal functions (that is, sine and cosine functions), the coefficients in the Karhunen\u2013Lo\u00e8ve theorem are random variables and the expansion basis depends on the process. In fact, the orthogonal basis functions used in this representation are determined by the covariance function of the process. One can think that the Karhunen\u2013Lo\u00e8ve transform adapts to the process in order to produce the best possible basis for its expansion.\nIn the case of a centered stochastic process {Xt}t \u2208 [a, b] (centered means E[Xt] = 0 for all t \u2208 [a, b]) satisfying a technical continuity condition, Xt admits a decomposition\nwhere Zk are pairwise uncorrelated random variables and the functions ek are continuous real-valued functions on [a, b] that are pairwise orthogonal in L2([a, b]). It is therefore sometimes said that the expansion is bi-orthogonal since the random coefficients Zk are orthogonal in the probability space while the deterministic functions ek are orthogonal in the time domain. The general case of a process Xt that is not centered can be brought back to the case of a centered process by considering Xt \u2212 E[Xt] which is a centered process.\nMoreover, if the process is Gaussian, then the random variables Zk are Gaussian and stochastically independent. This result generalizes the Karhunen\u2013Lo\u00e8ve transform. An important example of a centered real stochastic process on [0, 1] is the Wiener process; the Karhunen\u2013Lo\u00e8ve theorem can be used to provide a canonical orthogonal representation for it. In this case the expansion consists of sinusoidal functions.\nThe above expansion into uncorrelated random variables is also known as the Karhunen\u2013Lo\u00e8ve expansion or Karhunen\u2013Lo\u00e8ve decomposition. The empirical version (i.e., with the coefficients computed from a sample) is known as the Karhunen\u2013Lo\u00e8ve transform (KLT), principal component analysis, proper orthogonal decomposition (POD), Empirical orthogonal functions (a term used in meteorology and geophysics), or the Hotelling transform.\n\n\nTheorem. Let Xt be a zero-mean square-integrable stochastic process defined over a probability space (\u03a9, F, P) and indexed over a closed and bounded interval [a,\u00a0b], with continuous covariance function KX(s, t).\nThen KX(s,t) is a Mercer kernel and letting ek be an orthonormal basis on L2([a, b]) formed by the eigenfunctions of TKX with respective eigenvalues \u03bbk, Xt admits the following representation\nwhere the convergence is in L2, uniform in t and\nFurthermore, the random variables Zk have zero-mean, are uncorrelated and have variance \u03bbk\nNote that by generalizations of Mercer's theorem we can replace the interval [a, b] with other compact spaces C and the Lebesgue measure on [a, b] with a Borel measure whose support is C.\nSince the limit in the mean of jointly Gaussian random variables is jointly Gaussian, and jointly Gaussian random (centered) variables are independent if and only if they are orthogonal, we can also conclude:\nTheorem. The variables Zi have a joint Gaussian distribution and are stochastically independent if the original process {Xt}t is Gaussian.\nIn the Gaussian case, since the variables Zi are independent, we can say more:\nalmost surely.\nThis is a consequence of the independence of the Zk.\nIn the introduction, we mentioned that the truncated Karhunen\u2013Loeve expansion was the best approximation of the original process in the sense that it reduces the total mean-square error resulting of its truncation. Because of this property, it is often said that the KL transform optimally compacts the energy.\nMore specifically, given any orthonormal basis {fk} of L2([a, b]), we may decompose the process Xt as:\nwhere\nand we may approximate Xt by the finite sum\nfor some integer N.\nClaim. Of all such approximations, the KL approximation is the one that minimizes the total mean square error (provided we have arranged the eigenvalues in decreasing order).\nConsider the error resulting from the truncation at the N-th term in the following orthonormal expansion:\nThe mean-square error \u03b5N2(t) can be written as:\nWe then integrate this last equality over [a, b]. The orthonormality of the fk yields:\nThe problem of minimizing the total mean-square error thus comes down to minimizing the right hand side of this equality subject to the constraint that the fk be normalized. We hence introduce \u03b2k, the Lagrangian multipliers associated with these constraints, and aim at minimizing the following function:\nDifferentiating with respect to fi(t) (this is a functional derivative) and setting the derivative to 0 yields:\nwhich is satisfied in particular when\nIn other words, when the fk are chosen to be the eigenfunctions of TKX, hence resulting in the KL expansion.\nAn important observation is that since the random coefficients Zk of the KL expansion are uncorrelated, the Bienaym\u00e9 formula asserts that the variance of Xt is simply the sum of the variances of the individual components of the sum:\nIntegrating over [a, b] and using the orthonormality of the ek, we obtain that the total variance of the process is:\nIn particular, the total variance of the N-truncated approximation is\nAs a result, the N-truncated expansion explains\nof the variance; and if we are content with an approximation that explains, say, 95% of the variance, then we just have to determine an \n\n\n\nN\n\u2208\n\nN\n\n\n\n{\\displaystyle N\\in \\mathbb {N} }\n\n such that\nLet us consider a whole class of signals we want to approximate over the first M vectors of a basis. These signals are modeled as realizations of a random vector Y[n] of size N. To optimize the approximation we design a basis that minimizes the average approximation error. This section proves that optimal bases are Karhunen-Loeve bases that diagonalize the covariance matrix of Y. The random vector Y can be decomposed in an orthogonal basis\nas follows:\nwhere each\nis a random variable. The approximation from the first M \u2264 N vectors of the basis is\nThe energy conservation in an orthogonal basis implies\nThis error is related to the covariance of Y defined by\nFor any vector x[n] we denote by K the covariance operator represented by this matrix,\nThe error \u03b5[M] is therefore a sum of the last N \u2212 M coefficients of the covariance operator\nThe covariance operator K is Hermitian and Positive and is thus diagonalized in an orthogonal basis called a Karhunen-Lo\u00e8ve basis. The following theorem states that a Karhunen-Lo\u00e8ve basis is optimal for linear approximations.\nTheorem (Optimality of Karhunen-Lo\u00e8ve Basis). Let K be acovariance operator. For all M \u2265 1, the approximation error\nis minimum if and only if\nis a Karhunen-Loeve basis ordered by decreasing eigenvalues.\nLinear approximations project the signal on M vectors a priori. The approximation can be made more precise by choosing the M orthogonal vectors depending on the signal properties. This section analyzes the general performance of these non-linear approximations. A signal \n\n\n\nf\n\u2208\n\nH\n\n\n\n{\\displaystyle f\\in \\mathrm {H} }\n\n is approximated with M vectors selected adaptively in an orthonormal basis for \n\n\n\n\nH\n\n\n\n{\\displaystyle \\mathrm {H} }\n\n\nLet \n\n\n\n\nf\n\nM\n\n\n\n\n{\\displaystyle f_{M}}\n\n be the projection of f over M vectors whose indices are in IM:\nThe approximation error is the sum of the remaining coefficients\nTo minimize this error, the indices in IM must correspond to the M vectors having the largest inner product amplitude\nThese are the vectors that best correlate f. They can thus be interpreted as the main features of f. The resulting error is necessarily smaller than the error of a linear approximation which selects the M approximation vectors independently of f. Let us sort\nin decreasing order\nThe best non-linear approximation is\nIt can also be written as inner product thresholding:\nwith\nThe non-linear error is\nthis error goes quickly to zero as M increases, if the sorted values of \n\n\n\n\n|\n\u27e8\nf\n,\n\ng\n\n\nm\n\nk\n\n\n\n\n\u27e9\n|\n\n\n\n{\\displaystyle \\left|\\left\\langle f,g_{m_{k}}\\right\\rangle \\right|}\n\n have a fast decay as k increases. This decay is quantified by computing the \n\n\n\n\n\nI\n\n\n\nP\n\n\n\n\n\n{\\displaystyle \\mathrm {I} ^{\\mathrm {P} }}\n\n norm of the signal inner products in B:\nThe following theorem relates the decay of \u03b5[M] to \n\n\n\n\u2225\nf\n\n\u2225\n\n\nB\n\n,\np\n\n\n\n\n{\\displaystyle \\|f\\|_{\\mathrm {B} ,p}}\n\n\nTheorem (decay of error). If \n\n\n\n\u2225\nf\n\n\u2225\n\n\nB\n\n,\np\n\n\n<\n\u221e\n\n\n{\\displaystyle \\|f\\|_{\\mathrm {B} ,p}<\\infty }\n\n with p < 2 then\nand\nConversely, if \n\n\n\n\u03b5\n[\nM\n]\n=\no\n\n(\n\nM\n\n1\n\u2212\n\n\n2\np\n\n\n\n\n)\n\n\n\n{\\displaystyle \\varepsilon [M]=o\\left(M^{1-{\\frac {2}{p}}}\\right)}\n\n then\n\n\n\n\n\u2225\nf\n\n\u2225\n\n\nB\n\n,\nq\n\n\n<\n\u221e\n\n\n{\\displaystyle \\|f\\|_{\\mathrm {B} ,q}<\\infty }\n\n for any q > p.\nTo further illustrate the differences between linear and non-linear approximations, we study the decomposition of a simple non-Gaussian random vector in a Karhunen-Lo\u00e8ve basis. Processes whose realizations have a random translation are stationary. The Karhunen-Lo\u00e8ve basis is then a Fourier basis and we study its performance. To simplify the analysis, consider a random vector Y[n] of size N that is random shift modulo N of a deterministic signal f[n] of zero mean\nThe random shift P is uniformly distributed on [0,N-1]:\nClearly\nand\nHence\nSince RY is N periodic, Y is a circular stationary random vector. The covariance operator is a circular convolution with RY and is therefore diagonalized in the discrete Fourier Karhunen-Lo\u00e8ve basis\nThe power spectrum is Fourier Transform of RY:\nExample: Consider an extreme case where \n\n\n\nf\n[\nn\n]\n=\n\u03b4\n[\nn\n]\n\u2212\n\u03b4\n[\nn\n\u2212\n1\n]\n\n\n{\\displaystyle f[n]=\\delta [n]-\\delta [n-1]}\n\n. A theorem stated above guarantees that the Fourier Karhunen-Lo\u00e8ve basis produces a smaller expected approximation error than a canonical basis of Diracs \n\n\n\n\n\n{\n\ng\n\nm\n\n\n[\nn\n]\n=\n\u03b4\n[\nn\n\u2212\nm\n]\n}\n\n\n0\n\u2264\nm\n<\nN\n\n\n\n\n{\\displaystyle \\left\\{g_{m}[n]=\\delta [n-m]\\right\\}_{0\\leq m<N}}\n\n. Indeed, we do not know a priori the abscissa of the non-zero coefficients of Y, so there is no particular Dirac that is better adapted to perform the approximation. But the Fourier vectors cover the whole support of Y and thus absorb a part of the signal energy.\nSelecting higher frequency Fourier coefficients yields a better mean-square approximation than choosing a priori a few Dirac vectors to perform the approximation. The situation is totally different for non-linear approximations. If \n\n\n\nf\n[\nn\n]\n=\n\u03b4\n[\nn\n]\n\u2212\n\u03b4\n[\nn\n\u2212\n1\n]\n\n\n{\\displaystyle f[n]=\\delta [n]-\\delta [n-1]}\n\n then the discrete Fourier basis is extremely inefficient because f and hence Y have an energy that is almost uniformly spread among all Fourier vectors. In contrast, since f has only two non-zero coefficients in the Dirac basis, a non-linear approximation of Y with M \u2265 2 gives zero error.[6]\nWe have established the Karhunen\u2013Lo\u00e8ve theorem and derived a few properties thereof. We also noted that one hurdle in its application was the numerical cost of determining the eigenvalues and eigenfunctions of its covariance operator through the Fredholm integral equation of the second kind\nHowever, when applied to a discrete and finite process \n\n\n\n\n\n(\n\nX\n\nn\n\n\n)\n\n\nn\n\u2208\n{\n1\n,\n\u2026\n,\nN\n}\n\n\n\n\n{\\displaystyle \\left(X_{n}\\right)_{n\\in \\{1,\\ldots ,N\\}}}\n\n, the problem takes a much simpler form and standard algebra can be used to carry out the calculations.\nNote that a continuous process can also be sampled at N points in time in order to reduce the problem to a finite version.\nWe henceforth consider a random N-dimensional vector \n\n\n\nX\n=\n\n\n(\n\nX\n\n1\n\n\n\u00a0\n\nX\n\n2\n\n\n\u00a0\n\u2026\n\u00a0\n\nX\n\nN\n\n\n)\n\n\nT\n\n\n\n\n{\\displaystyle X=\\left(X_{1}~X_{2}~\\ldots ~X_{N}\\right)^{T}}\n\n. As mentioned above, X could contain N samples of a signal but it can hold many more representations depending on the field of application. For instance it could be the answers to a survey or economic data in an econometrics analysis.\nAs in the continuous version, we assume that X is centered, otherwise we can let \n\n\n\nX\n:=\nX\n\u2212\n\n\u03bc\n\nX\n\n\n\n\n{\\displaystyle X:=X-\\mu _{X}}\n\n (where \n\n\n\n\n\u03bc\n\nX\n\n\n\n\n{\\displaystyle \\mu _{X}}\n\n is the mean vector of X) which is centered.\nLet us adapt the procedure to the discrete case.\nRecall that the main implication and difficulty of the KL transformation is computing the eigenvectors of the linear operator associated to the covariance function, which are given by the solutions to the integral equation written above.\nDefine \u03a3, the covariance matrix of X, as an N \u00d7 N matrix whose elements are given by:\nRewriting the above integral equation to suit the discrete case, we observe that it turns into:\nwhere \n\n\n\ne\n=\n(\n\ne\n\n1\n\n\n\u00a0\n\ne\n\n2\n\n\n\u00a0\n\u2026\n\u00a0\n\ne\n\nN\n\n\n\n)\n\nT\n\n\n\n\n{\\displaystyle e=(e_{1}~e_{2}~\\ldots ~e_{N})^{T}}\n\n is an N-dimensional vector.\nThe integral equation thus reduces to a simple matrix eigenvalue problem, which explains why the PCA has such a broad domain of applications.\nSince \u03a3 is a positive definite symmetric matrix, it possesses a set of orthonormal eigenvectors forming a basis of \n\n\n\n\n\nR\n\n\nN\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{N}}\n\n, and we write \n\n\n\n{\n\n\u03bb\n\ni\n\n\n,\n\n\u03d5\n\ni\n\n\n\n}\n\ni\n\u2208\n{\n1\n,\n\u2026\n,\nN\n}\n\n\n\n\n{\\displaystyle \\{\\lambda _{i},\\phi _{i}\\}_{i\\in \\{1,\\ldots ,N\\}}}\n\n this set of eigenvalues and corresponding eigenvectors, listed in decreasing values of \u03bbi. Let also \u03a6 be the orthonormal matrix consisting of these eigenvectors:\nIt remains to perform the actual KL transformation, called the principal component transform in this case. Recall that the transform was found by expanding the process with respect to the basis spanned by the eigenvectors of the covariance function. In this case, we hence have:\nIn a more compact form, the principal component transform of X is defined by:\nThe i-th component of Y is \n\n\n\n\nY\n\ni\n\n\n=\n\n\u03d5\n\ni\n\n\nT\n\n\nX\n\n\n{\\displaystyle Y_{i}=\\phi _{i}^{T}X}\n\n, the projection of X on \n\n\n\n\n\u03d5\n\ni\n\n\n\n\n{\\displaystyle \\phi _{i}}\n\n and the inverse transform X = \u03a6Y yields the expansion of X on the space spanned by the \n\n\n\n\n\u03d5\n\ni\n\n\n\n\n{\\displaystyle \\phi _{i}}\n\n:\nAs in the continuous case, we may reduce the dimensionality of the problem by truncating the sum at some \n\n\n\nK\n\u2208\n{\n1\n,\n\u2026\n,\nN\n}\n\n\n{\\displaystyle K\\in \\{1,\\ldots ,N\\}}\n\n such that\nwhere \u03b1 is the explained variance threshold we wish to set.\nWe can also reduce the dimensionality through the use of multilevel dominant eigenvector estimation (MDEE).[7]\nThere are numerous equivalent characterizations of the Wiener process which is a mathematical formalization of Brownian motion. Here we regard it as the centered standard Gaussian process Wt with covariance function\nWe restrict the time domain to [a, b]=[0,1] without loss of generality.\nThe eigenvectors of the covariance kernel are easily determined. These are\nand the corresponding eigenvalues are\nIn order to find the eigenvalues and eigenvectors, we need to solve the integral equation:\ndifferentiating once with respect to t yields:\na second differentiation produces the following differential equation:\nThe general solution of which has the form:\nwhere A and B are two constants to be determined with the boundary conditions. Setting t=0 in the initial integral equation gives e(0)=0 which implies that B=0 and similarly, setting t=1 in the first differentiation yields e' (1)=0, whence:\nwhich in turn implies that eigenvalues of TKX are:\nThe corresponding eigenfunctions are thus of the form:\nA is then chosen so as to normalize ek:\nThis gives the following representation of the Wiener process:\nTheorem. There is a sequence {Zi}i of independent Gaussian random variables with mean zero and variance 1 such that\nNote that this representation is only valid for \n\n\n\nt\n\u2208\n[\n0\n,\n1\n]\n.\n\n\n{\\displaystyle t\\in [0,1].}\n\n On larger intervals, the increments are not independent. As stated in the theorem, convergence is in the L2 norm and uniform in t.\nSimilarly the Brownian bridge \n\n\n\n\nB\n\nt\n\n\n=\n\nW\n\nt\n\n\n\u2212\nt\n\nW\n\n1\n\n\n\n\n{\\displaystyle B_{t}=W_{t}-tW_{1}}\n\n which is a stochastic process with covariance function\ncan be represented as the series\nAdaptive optics systems sometimes use K\u2013L functions to reconstruct wave-front phase information (Dai 1996, JOSA A).\nKarhunen\u2013Lo\u00e8ve expansion is closely related to the Singular Value Decomposition. The latter has myriad applications in image processing, radar, seismology, and the like. If one has independent vector observations from a vector valued stochastic process then the left singular vectors are maximum likelihood estimates of the ensemble KL expansion.\nIn communication, we usually have to decide whether a signal from a noisy channel contains valuable information. The following hypothesis testing is used for detecting continuous signal s(t) from channel output X(t), N(t) is the channel noise, which is usually assumed zero mean gaussian process with correlation function \n\n\n\n\nR\n\nN\n\n\n(\nt\n,\ns\n)\n=\nE\n[\nN\n(\nt\n)\nN\n(\ns\n)\n]\n\n\n{\\displaystyle R_{N}(t,s)=E[N(t)N(s)]}\n\n\nWhen the channel noise is white, its correlation function is\nand it has constant power spectrum density. In physically practical channel, the noise power is finite, so:\nThen the noise correlation function is sinc function with zeros at \n\n\n\n\n\nn\n\n2\n\u03c9\n\n\n\n,\nn\n\u2208\n\nZ\n\n.\n\n\n{\\displaystyle {\\frac {n}{2\\omega }},n\\in \\mathbf {Z} .}\n\n Since are uncorrelated and gaussian, they are independent. Thus we can take samples from X(t) with time spacing\nLet \n\n\n\n\nX\n\ni\n\n\n=\nX\n(\ni\n\u0394\nt\n)\n\n\n{\\displaystyle X_{i}=X(i\\Delta t)}\n\n. We have a total of \n\n\n\nn\n=\n\n\nT\n\n\u0394\nt\n\n\n\n=\nT\n(\n2\n\u03c9\n)\n=\n2\n\u03c9\nT\n\n\n{\\displaystyle n={\\frac {T}{\\Delta t}}=T(2\\omega )=2\\omega T}\n\n i.i.d samples \n\n\n\n{\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n,\n.\n.\n.\n,\n\nX\n\nn\n\n\n}\n\n\n{\\displaystyle \\{X_{1},X_{2},...,X_{n}\\}}\n\n to develop the likelihood-ratio test. Define signal \n\n\n\n\nS\n\ni\n\n\n=\nS\n(\ni\n\u0394\nt\n)\n\n\n{\\displaystyle S_{i}=S(i\\Delta t)}\n\n, the problem becomes,\nThe log-likelihood ratio\nAs t \u2192 0, let:\nThen G is the test statistics and the Neyman\u2013Pearson optimum detector is\nAs G is gaussian, we can characterize it by finding its mean and variances. Then we get\nwhere\nis the signal energy.\nThe false alarm error\nAnd the probability of detection:\nwhere \u03a6 is the cdf of standard normal gaussian variable.\nWhen N(t) is colored (correlated in time) gaussian noise with zero mean and covariance function \n\n\n\n\nR\n\nN\n\n\n(\nt\n,\ns\n)\n=\nE\n[\nX\n(\nt\n)\nX\n(\ns\n)\n]\n,\n\n\n{\\displaystyle R_{N}(t,s)=E[X(t)X(s)],}\n\n we cannot sample independent discrete observations by evenly spacing the time. Instead, we can use K\u2013L expansion to uncorrelate the noise process and get independent gaussian observation 'samples'. The K\u2013L expansion of N(t):\nwhere \n\n\n\n\nN\n\ni\n\n\n=\n\u222b\nN\n(\nt\n)\n\n\u03a6\n\ni\n\n\n(\nt\n)\nd\nt\n\n\n{\\displaystyle N_{i}=\\int N(t)\\Phi _{i}(t)dt}\n\n and the orthonormal bases \n\n\n\n{\n\n\u03a6\n\ni\n\n\n\nt\n\n}\n\n\n{\\displaystyle \\{\\Phi _{i}{t}\\}}\n\n are generated by kernel \n\n\n\n\nR\n\nN\n\n\n(\nt\n,\ns\n)\n\n\n{\\displaystyle R_{N}(t,s)}\n\n, i.e., solution to\nDo the expansion:\nwhere \n\n\n\n\nS\n\ni\n\n\n=\n\n\u222b\n\n0\n\n\nT\n\n\nS\n(\nt\n)\n\n\u03a6\n\ni\n\n\n(\nt\n)\nd\nt\n,\n0\n<\nt\n<\nT\n.\n\n\n{\\displaystyle S_{i}=\\int _{0}^{T}S(t)\\Phi _{i}(t)dt,0<t<T.}\n\n, then\nunder H and \n\n\n\n\nN\n\ni\n\n\n+\n\nS\n\ni\n\n\n\n\n{\\displaystyle N_{i}+S_{i}}\n\n under K. Let \n\n\n\n\n\nX\n\u00af\n\n\n=\n{\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n,\n\u2026\n}\n\n\n{\\displaystyle {\\overline {X}}=\\{X_{1},X_{2},\\dots \\}}\n\n, we have\nHence, the log-LR is given by\nand the optimum detector is\nDefine\nthen \n\n\n\nG\n=\n\n\u222b\n\n0\n\n\nT\n\n\nk\n(\nt\n)\nx\n(\nt\n)\nd\nt\n\n\n{\\displaystyle G=\\int _{0}^{T}k(t)x(t)dt}\n\n.\nSince\nk(t) is the solution to\nIf N(t)is wide-sense stationary,\nwhich is known as the Wiener\u2013Hopf equation. The equation can be solved by taking fourier transform, but not practically realizable since infinite spectrum needs spatial factorization. A special case which is easy to calculate k(t) is white gaussian noise.\nThe corresponding impulse response is h(t) = k(T-t) = C S(T-t). Let C = 1, this is just the result we arrived at in previous section for detecting of signal in white noise.\nSince X(t) is a gaussian process,\nis a gaussian random variable that can be characterized by its mean and variance.\nHence, we obtain the distributions of H and K:\nThe false alarm error is\nSo the test threshold for the Neyman\u2013Pearson optimum detector is\nIts power of detection is\nWhen the noise is white gaussian process, the signal power is\nFor some type of colored noise, a typical practise is to add a prewhitening filter before the matched filter to transform the colored noise into white noise. For example, N(t) is a wide-sense stationary colored noise with correlation function\nThe transfer function of prewhitening filter is\nWhen the signal we want to detect from the noisy channel is also random, for example, a white gaussian process X(t), we can still implement K\u2013L expansion to get independent sequence of observation. In this case, the detection problem is described as follows:\nX(t) is a random process with correlation function \n\n\n\n\nR\n\nX\n\n\n(\nt\n,\ns\n)\n=\nE\n{\nX\n[\nt\n]\nX\n[\ns\n]\n}\n\n\n{\\displaystyle R_{X}(t,s)=E\\{X[t]X[s]\\}}\n\n\nThe K\u2013L expansion of X(t) is\nwhere\nare solutions to\nSo \n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n's are independent sequence of r.v's with zero mean and variance \n\n\n\n\n\u03bb\n\ni\n\n\n\n\n{\\displaystyle \\lambda _{i}}\n\n. Expanding Y(t) and N(t) by \n\n\n\n\n\u03a6\n\ni\n\n\n(\nt\n)\n\n\n{\\displaystyle \\Phi _{i}(t)}\n\n, we get\nwhere\nAs N(t) is gaussian white noise, \n\n\n\n\nN\n\ni\n\n\n\n\n{\\displaystyle N_{i}}\n\n's are i.i.d sequence of r.v with zero mean and variance \n\n\n\n\n\n\n1\n2\n\n\n\n\nN\n\n0\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}N_{0}}\n\n, then the problem is simplified as follows,\nThe Neyman\u2013Pearson optimal test:\nso the log-likelihood ratio\nSince\nis just the minimum-mean-square estimate of \n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n given \n\n\n\n\nY\n\ni\n\n\n\n\n{\\displaystyle Y_{i}}\n\n's,\nK\u2013L expansion has the following property: If\nwhere\nthen\nSo let\nNoncausal filter Q(t, s) can be used to get the estimate through\nBy orthogonality principle, Q(t,s) satisfies\nHowever, for practical reason, it's necessary to further derive the causal filter h(t, s), where h(t, s) = 0 for s > t, to get estimate \n\n\n\n\n\n\n\nX\n(\nt\n\n|\n\nt\n)\n\n^\n\n\n\n\n\n{\\displaystyle {\\hat {X(t|t)}}}\n\n. Specifically,", 
    "dbpedia_url": "http://dbpedia.org/resource/Karhunen\u2013Lo\u00e8ve_theorem", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Karhunen\u2013Lo\u00e8ve_theorem\n"
}