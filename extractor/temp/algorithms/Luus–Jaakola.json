{
    "about": "In computational engineering, Luus\u2013Jaakola (LJ) denotes a heuristic for global optimization of a real-valued function.[1] In engineering use, LJ is not an algorithm that terminates with an optimal solution; nor is it an iterative method that generates a sequence of points that converges to an optimal solution (when one exists). However, when applied to a twice continuously differentiable function, the LJ heuristic is a proper iterative method, that generates a sequence that has a convergent subsequence; for this class of problems, Newton's method is recommended and enjoys a quadratic rate of convergence, while no convergence rate analysis has been given for the LJ heuristic.[1] In practice, the LJ heuristic has been recommended for functions that need be neither convex nor differentiable nor locally Lipschitz: The LJ heuristic does not use a gradient or subgradient when one be available, which allows its application to non-differentiable and non-convex problems.", 
    "name": "Luus\u2013Jaakola", 
    "classification": "Optimization Algorithms And Methods", 
    "full_text": "In computational engineering, Luus\u2013Jaakola (LJ) denotes a heuristic for global optimization of a real-valued function.[1] In engineering use, LJ is not an algorithm that terminates with an optimal solution; nor is it an iterative method that generates a sequence of points that converges to an optimal solution (when one exists). However, when applied to a twice continuously differentiable function, the LJ heuristic is a proper iterative method, that generates a sequence that has a convergent subsequence; for this class of problems, Newton's method is recommended and enjoys a quadratic rate of convergence, while no convergence rate analysis has been given for the LJ heuristic.[1] In practice, the LJ heuristic has been recommended for functions that need be neither convex nor differentiable nor locally Lipschitz: The LJ heuristic does not use a gradient or subgradient when one be available, which allows its application to non-differentiable and non-convex problems.\nProposed by Luus and Jaakola,[2] LJ generates a sequence of iterates. The next iterate is selected from a sample from a neighborhood of the current position using a uniform distribution. With each iteration, the neighborhood decreases, which forces a subsequence of iterates to converge to a cluster point.[1]\nLuus has applied LJ in optimal control,[3] transformer design,[4] metallurgical processes,[5] and chemical engineering.[6]\n\n\nAt each step, the LJ heuristic maintains a box from which it samples points randomly, using a uniform distribution on the box. For a unimodal function, the probability of reducing the objective function decreases as the box approach a minimum. The picture displays a one-dimensional example.\nLet f:\u00a0\u211dn\u00a0\u2192 \u211d be the fitness or cost function which must be minimized. Let x\u00a0\u2208\u00a0\u211dn designate a position or candidate solution in the search-space. The LJ heuristic iterates the following steps:\nNair proved a convergence analysis. For twice continuously differentiable functions, the LJ heuristic generates a sequence of iterates having a convergent subsequence.[1] For this class of problems, Newton's method is the usual optimization method, and it has quadratic convergence (regardless of the dimension of the space, which can be a Banach space, according to Kantorovich's analysis).\nThe worst-case complexity of minimization on the class of unimodal functions grows exponentially in the dimension of the problem, according to the analysis of Yudin and Nemirovsky, however. The Yudin-Nemirovsky analysis implies that no method can be fast on high-dimensional problems that lack convexity:\n\"The catastrophic growth [in the number of iterations needed to reach an approximate solution of a given accuracy] as [the number of dimensions increases to infinity] shows that it is meaningless to pose the question of constructing universal methods of solving ... problems of any appreciable dimensionality 'generally'. It is interesting to note that the same [conclusion] holds for ... problems generated by uni-extremal [that is, unimodal] (but not convex) functions.\"[7]\nWhen applied to twice continuously differentiable problems, the LJ heuristic's rate of convergence decreases as the number of dimensions increases.[8]\nPage 7 summarizes the later discussion of Nemirovksy & Yudin (1983, pp.\u00a036\u201339): Nemirovsky, A. S.; Yudin, D. B. (1983). Problem complexity and method efficiency in optimization. Wiley-Interscience Series in Discrete Mathematics (Translated by E. R. Dawson from the (1979) Russian (Moscow: Nauka) ed.). New York: John Wiley & Sons, Inc. pp.\u00a0xv+388. ISBN\u00a00-471-10345-4. MR\u00a0702836.\u00a0\nNemirovsky, A. S.; Yudin, D. B. (1983). Problem complexity and method efficiency in optimization. Wiley-Interscience Series in Discrete Mathematics (Translated by E. R. Dawson from the (1979) Russian (Moscow: Nauka) ed.). New York: John Wiley & Sons, Inc. pp.\u00a0xv+388. ISBN\u00a00-471-10345-4. MR\u00a0702836.\u00a0", 
    "dbpedia_url": "http://dbpedia.org/resource/Luus\u2013Jaakola", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Luus\u2013Jaakola\n"
}