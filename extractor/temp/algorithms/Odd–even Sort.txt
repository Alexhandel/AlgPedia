ABOUT
In computing, an odd–even sort or odd–even transposition sort (also known as brick sort[1][self-published source]) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections. It is a comparison sort related to bubble sort, with which it shares many characteristics. It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched. The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.
FULL TEXT
In computing, an odd–even sort or odd–even transposition sort (also known as brick sort[1][self-published source]) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections. It is a comparison sort related to bubble sort, with which it shares many characteristics. It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched. The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.


On parallel processors, with one value per processor and only local left–right neighbor connections, the processors all concurrently do a compare–exchange operation with their neighbors, alternating between odd–even and even–odd pairings. This algorithm was originally presented, and shown to be efficient on such processors, by Habermann in 1972.[2]
The algorithm extends efficiently to the case of multiple items per processor. In the Baudet–Stevenson odd–even merge-splitting algorithm, each processor sorts its own sublist at each step, using any efficient sort algorithm, and then performs a merge splitting, or transposition–merge, operation with its neighbor, with neighbor pairing alternating between odd–even and even–odd on each step.[3]
A related but more efficient sort algorithm is the Batcher odd–even mergesort, using compare–exchange operations and perfect-shuffle operations.[4] Batcher's method is efficient on parallel processors with long-range connections.[5]
The single-processor algorithm, like bubblesort, is simple but not very efficient. Here a zero-based index is assumed:
This is an example of the algorithm in C++:
This is an example of the algorithm in php:
This is an example of the algorithm in python:
This is an example of the algorithm in MATLAB/OCTAVE:
Claim: Let 




a

1


,
.
.
.
,

a

n




{\displaystyle a_{1},...,a_{n}}

 be a sequence of data ordered by <. The odd-even sort algorithm correctly sorts this data in 



n


{\displaystyle n}

 passes. (A pass here is defined to be a full sequence of odd-even, or even-odd comparisons. The passes occur in order pass 1: odd-even, pass 2: even-odd, etc.)
Proof:
This proof is based loosely on one by Thomas Worsch.[6]
Since the sorting algorithm only involves comparison-swap operations and is oblivious (the order of comparison-swap operations does not depend on the data), by Knuth's 0-1 sorting principle,[7][8] it suffices to check correctness when each 




a

i




{\displaystyle a_{i}}

 is either 0 or 1. Assume that there are 



e


{\displaystyle e}

 1's.
Observe that the rightmost 1 can be either in an even or odd position, so it might not be moved by the first odd-even pass. But after the first odd-even pass, the rightmost 1 will be in an even position. It follows that it will be moved to the right by all remaining passes. Since the rightmost one starts in position greater than or equal to 



e


{\displaystyle e}

, it must be moved at most 



n
−
e


{\displaystyle n-e}

 steps. It follows that it takes at most 



n
−
e
+
1


{\displaystyle n-e+1}

 passes to move the rightmost 1 to its correct position.
Now, consider the second rightmost 1. After two passes, the 1 to its right will have moved right by at least one step. It follows that, for all remaining passes, we can view the second rightmost 1 as the rightmost 1. The second rightmost 1 starts in position at least 



e
−
1


{\displaystyle e-1}

 at must be moved to position at most 



n
−
1


{\displaystyle n-1}

, so it must be moved at most 



(
n
−
1
)
−
(
e
−
1
)
=
n
−
e


{\displaystyle (n-1)-(e-1)=n-e}

 steps. After at most 2 passes, the rightmost 1 will have already moved, so the entry to the right of the second rightmost 1 will be 0. Hence, for all passes after the first two, the second rightmost 1 will move to the right. It thus takes at most 



n
−
e
+
2


{\displaystyle n-e+2}

 passes to move the second rightmost 1 to its correct position.
Continuing in this manner, by induction it can be shown that the 



i


{\displaystyle i}

-th rightmost 1 is moved to its correct position in at most 



n
−
e
+
i
+
1


{\displaystyle n-e+i+1}

 passes. It follows that the 



e


{\displaystyle e}

-th rightmost 1 is moved to its correct position in at most 



n
−
e
+
(
e
−
1
)
+
1
=
n


{\displaystyle n-e+(e-1)+1=n}

 passes (consider: counting starts at value "0"). The list is thus correctly sorted in 



n


{\displaystyle n}

 passes. QED.
We remark that each pass takes O(n) steps, so this algorithm is O(n^2) complexity.