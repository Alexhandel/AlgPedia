{
    "about": "The Sch\u00f6nhage\u2013Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Sch\u00f6nhage and Volker Strassen in 1971.[1] The run-time bit complexity is, in Big O notation,\n\n\n\nO\n\n\n(\n\n\nn\n\u22c5\nlog\n\u2061\n(\nn\n)\n\u22c5\nlog\n\u2061\n\n\n(\n\n\nlog\n\u2061\n(\nn\n)\n\n\n)\n\n\n\n\n)\n\n\n\n\n{\\displaystyle O{\\Big (}n\\cdot \\log(n)\\cdot \\log {\\big (}\\log(n){\\big )}{\\Big )}}\n\n for two n-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 2n+1 elements, a specific type of number theoretic transform.", 
    "name": "Sch\u00f6Nhage\u2013Strassen Algorithm", 
    "classification": "Computer Arithmetic Algorithms", 
    "full_text": "The Sch\u00f6nhage\u2013Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Sch\u00f6nhage and Volker Strassen in 1971.[1] The run-time bit complexity is, in Big O notation,\n\n\n\nO\n\n\n(\n\n\nn\n\u22c5\nlog\n\u2061\n(\nn\n)\n\u22c5\nlog\n\u2061\n\n\n(\n\n\nlog\n\u2061\n(\nn\n)\n\n\n)\n\n\n\n\n)\n\n\n\n\n{\\displaystyle O{\\Big (}n\\cdot \\log(n)\\cdot \\log {\\big (}\\log(n){\\big )}{\\Big )}}\n\n for two n-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 2n+1 elements, a specific type of number theoretic transform.\nThe Sch\u00f6nhage\u2013Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, F\u00fcrer's algorithm, was announced with lower asymptotic complexity;[2] however, F\u00fcrer's algorithm currently only achieves an advantage for astronomically large values and is not used in practice.\nIn practice the Sch\u00f6nhage\u2013Strassen algorithm starts to outperform older methods such as Karatsuba and Toom\u2013Cook multiplication for numbers beyond 2215 to 2217 (10,000 to 40,000 decimal digits).[3][4][5] The GNU Multi-Precision Library uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture.[6] There is a Java implementation of Sch\u00f6nhage\u2013Strassen which uses it above 74,000 decimal digits.[7]\nApplications of the Sch\u00f6nhage\u2013Strassen algorithm include mathematical empiricism, such as the Great Internet Mersenne Prime Search and computing approximations of \u03c0, as well as practical applications such as Kronecker substitution, in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; this is used in practice by GMP-ECM for Lenstra elliptic curve factorization.[8]\n\n\nThis section explains in detail how Sch\u00f6nhage\u2013Strassen is implemented. It is based primarily on an overview of the method by Crandall and Pomerance in their Prime Numbers: A Computational Perspective.[9] This variant differs somewhat from Sch\u00f6nhage's original method in that it exploits the discrete weighted transform to perform negacyclic convolutions more efficiently. Another source for detailed information is Knuth's The Art of Computer Programming.[10]\nSuppose we are multiplying two numbers like 123 and 456 using long multiplication with base B digits, but without performing any carrying. The result might look something like this:\nThis sequence (4, 13, 28, 27, 18) is called the acyclic or linear convolution of the two original sequences (1,2,3) and (4,5,6). Once you have the acyclic convolution of two sequences, computing the product of the original numbers is easy: you just perform the carrying (for example, in the rightmost column, you'd keep the 8 and add the 1 to the column containing 27). In the example this yields the correct product 56088.\nThere are two other types of convolutions that will be useful. Suppose the input sequences have n elements (here 3). Then the acyclic convolution has n+n\u22121 elements; if we take the rightmost n elements and add the leftmost n\u22121 elements, this produces the cyclic convolution:\nIf we perform carrying on the cyclic convolution, the result is equivalent to the product of the inputs mod Bn\u00a0\u2212\u00a01. In the example, 103\u00a0\u2212\u00a01 = 999, performing carrying on (28, 31, 31) yields 3141, and 3141 \u2261 56088 (mod 999).\nConversely, if we take the rightmost n elements and subtract the leftmost n\u22121 elements, this produces the negacyclic convolution:\nIf we perform carrying on the negacyclic convolution, the result is equivalent to the product of the inputs mod Bn\u00a0+\u00a01. In the example, 103\u00a0+\u00a01 = 1001, performing carrying on (28, 23, 5) yields 3035, and 3035 \u2261 56088 (mod 1001). The negacyclic convolution can contain negative numbers, which can be eliminated during carrying using borrowing, as is done in long subtraction.\nLike other multiplication methods based on the Fast Fourier transform, Sch\u00f6nhage\u2013Strassen depends fundamentally on the convolution theorem, which provides an efficient way to compute the cyclic convolution of two sequences. It states that:\nOr in symbols:\nIf we compute the DFT and IDFT using a fast Fourier transform algorithm, and invoke our multiplication algorithm recursively to multiply the entries of the transformed vectors DFT(X) and DFT(Y), this yields an efficient algorithm for computing the cyclic convolution.\nIn this algorithm, it will be more useful to compute the negacyclic convolution; as it turns out, a slightly modified version of the convolution theorem (see discrete weighted transform) can enable this as well. Suppose the vectors X and Y have length n, and a is a primitive root of unity of order 2n (that is, a2n = 1 and a to all smaller powers is not 1). Then we can define a third vector A, called the weight vector, as:\nNow, we can state:\nIn other words, it's the same as before except that the inputs are first multiplied by A, and the result is multiplied by A\u22121.\nThe discrete Fourier transform is an abstract operation that can be performed in any algebraic ring; typically it's performed in the complex numbers, but actually performing complex arithmetic to sufficient precision to ensure accurate results for multiplication is slow and error-prone. Instead, we will use the approach of the number theoretic transform, which is to perform the transform in the integers mod N for some integer N.\nJust like there are primitive roots of unity of every order in the complex plane, given any order n we can choose a suitable N such that b is a primitive root of unity of order n in the integers mod N (in other words, bn \u2261 1 (mod N), and no smaller power of b is equivalent to 1 mod N).\nThe algorithm will spend most of its time performing recursive multiplications of smaller numbers; with a naive algorithm, these occur in a number of places:\nThe key insight to Sch\u00f6nhage\u2013Strassen is to choose N, the modulus, to be equal to 2n\u00a0+\u00a01 for some integer n that is a multiple of the number of pieces P=2p being transformed. This has a number of benefits in standard systems that represent large integers in binary form:\nTo make the recursive multiplications convenient, we will frame Sch\u00f6nhage\u2013Strassen as being a specialized multiplication algorithm for computing not just the product of two numbers, but the product of two numbers mod 2n\u00a0+\u00a01 for some given n. This is not a loss of generality, since one can always choose n large enough so that the product mod 2n\u00a0+\u00a01 is simply the product.\nIn the course of the algorithm, there are many cases in which multiplication or division by a power of two (including all roots of unity) can be profitably replaced by a small number of shifts and adds. This makes use of the observation that:\nNote that a k-digit number in base 2n written in positional notation can be expressed as \n\n\n\n(\n\nd\n\nk\n\u2212\n1\n\n\n,\n\u2026\n,\n\nd\n\n1\n\n\n,\n\nd\n\n0\n\n\n)\n\n\n{\\displaystyle (d_{k-1},\\dots ,d_{1},d_{0})}\n\n. It represents the number \n\n\n\n\n\u2211\n\ni\n=\n0\n\n\nk\n\u2212\n1\n\n\n\nd\n\ni\n\n\n\u22c5\n(\n\n2\n\nn\n\n\n\n)\n\ni\n\n\n\n\n{\\displaystyle \\sum _{i=0}^{k-1}d_{i}\\cdot (2^{n})^{i}}\n\n. Also note that for each \n\n\n\n\nd\n\ni\n\n\n\n\n{\\displaystyle d_{i}}\n\n, \n\n\n\n0\n\u2264\n\nd\n\ni\n\n\n<\n\n2\n\nn\n\n\n\n\n{\\displaystyle 0\\leq d_{i}<2^{n}}\n\n.\nThis makes it simple to reduce a number represented in binary mod 2n\u00a0+\u00a01: take the rightmost (least significant) n bits, subtract the next n bits, add the next n bits, and so on until the bits are exhausted. If the resulting value is still not between 0 and 2n, normalize it by adding or subtracting a multiple of the modulus 2n\u00a0+\u00a01. For example, if n=3 (and so the modulus is 23+1 = 9) and the number being reduced is 656, we have:\nMoreover, it's possible to effect very large shifts without ever constructing the shifted result. Suppose we have a number A between 0 and 2n, and wish to multiply it by 2k. Dividing k by n we find k = qn + r with r < n. It follows that:\nSince A is \u2264 2n and r < n, A shift-left r has at most 2n\u22121 bits, and so only one shift and subtraction (followed by normalization) is needed.\nFinally, to divide by 2k, observe that squaring the first equivalence above yields:\nHence,\nThe algorithm follows a split, evaluate (forward FFT), pointwise multiply, interpolate (inverse FFT), and combine phases similar to Karatsuba and Toom-Cook methods.\nGiven input numbers x and y, and an integer N, the following algorithm computes the product xy mod 2N\u00a0+\u00a01. Provided N is sufficiently large this is simply the product.\nThe optimal number of pieces to divide the input into is proportional to \n\n\n\n\n\nN\n\n\n\n\n{\\displaystyle {\\sqrt {N}}}\n\n, where N is the number of input bits, and this setting achieves the running time of O(N log N log log N),[1][9] so the parameter k should be set accordingly. In practice, it is set empirically based on the input sizes and the architecture, typically to a value between 4 and 16.[8]\nIn step 2, the observation is used that:\n\nThis section explains a number of important practical optimizations that have been considered when implementing Sch\u00f6nhage\u2013Strassen in real systems. It is based primarily on a 2007 work by Gaudry, Kruppa, and Zimmermann describing enhancements to the GNU Multi-Precision Library.[8]\nBelow a certain cutoff point, it's more efficient to perform the recursive multiplications using other algorithms, such as Toom\u2013Cook multiplication. The results must be reduced mod 2n\u00a0+\u00a01, which can be done efficiently as explained above in Shift optimizations with shifts and adds/subtracts.\nComputing the IDFT involves dividing each entry by the primitive root of unity 22n/2k, an operation that is frequently combined with multiplying the vector by A\u22121 afterwards, since both involve division by a power of two.\nIn a system where a large number is represented as an array of 2w-bit words, it's useful to ensure that the vector size 2k is also a multiple of the bits per word by choosing k \u2265 w (e.g. choose k \u2265 5 on a 32-bit computer and k \u2265 6 on a 64-bit computer); this allows the inputs to be broken up into pieces without bit shifts, and provides a uniform representation for values mod 2n\u00a0+\u00a01 where the high word can only be zero or one.\nNormalization involves adding or subtracting the modulus 2n+1; this value has only two bits set, which means this can be done in constant time on average with a specialized operation.\nIterative FFT algorithms such as the Cooley\u2013Tukey FFT algorithm, although frequently used for FFTs on vectors of complex numbers, tend to exhibit very poor cache locality with the large vector entries used in Sch\u00f6nhage\u2013Strassen. The straightforward recursive, not in-place implementation of FFT is more successful, with all operations fitting in the cache beyond a certain point in the call depth, but still makes suboptimal use of the cache in higher call depths. Gaudry, Kruppa, and Zimmerman used a technique combining Bailey's 4-step algorithm with higher radix transforms that combine multiple recursive steps. They also mix phases, going as far into the algorithm as possible on each element of the vector before moving on to the next one.\nThe \"square root of 2 trick\", first described by Sch\u00f6nhage, is to note that, provided k \u2265 2, 23n/4\u22122n/4 is a square root of 2 mod 2n+1, and so a 4n-th root of unity (since 22n \u2261 1). This allows the transform length to be extended from 2k to 2k + 1.\nFinally, the authors are careful to choose the right value of k for different ranges of input numbers, noting that the optimal value of k may go back and forth between the same values several times as the input size increases.", 
    "dbpedia_url": "http://dbpedia.org/resource/Sch\u00f6nhage\u2013Strassen_algorithm", 
    "wikipedia_url": "http://en.wikipedia.org/wiki/Sch\u00f6nhage\u2013Strassen_algorithm\n"
}